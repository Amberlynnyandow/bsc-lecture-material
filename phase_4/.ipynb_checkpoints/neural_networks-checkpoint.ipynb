{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Wait,-Wait,-Wait...-Why-a-Neural-Network?\" data-toc-modified-id=\"Wait,-Wait,-Wait...-Why-a-Neural-Network?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Wait, Wait, Wait... Why a Neural Network?</a></span></li><li><span><a href=\"#Starting-with-a-Perceptron\" data-toc-modified-id=\"Starting-with-a-Perceptron-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Starting with a Perceptron</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-Diagram\" data-toc-modified-id=\"A-Diagram-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>A Diagram</a></span></li><li><span><a href=\"#A-Scenario---Logic\" data-toc-modified-id=\"A-Scenario---Logic-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>A Scenario - Logic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logical-AND\" data-toc-modified-id=\"Logical-AND-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Logical <code>AND</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution\" data-toc-modified-id=\"Solution-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Solution</a></span></li></ul></li><li><span><a href=\"#Logical-OR\" data-toc-modified-id=\"Logical-OR-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Logical <code>OR</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution\" data-toc-modified-id=\"Solution-2.2.2.1\"><span class=\"toc-item-num\">2.2.2.1&nbsp;&nbsp;</span>Solution</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Neural-Networks-Overview\" data-toc-modified-id=\"Neural-Networks-Overview-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Neural Networks Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Couple-ways-to-think-of-neural-networks\" data-toc-modified-id=\"Couple-ways-to-think-of-neural-networks-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Couple ways to think of neural networks</a></span></li><li><span><a href=\"#Parts-of-a-Neural-Network\" data-toc-modified-id=\"Parts-of-a-Neural-Network-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Parts of a Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Layers\" data-toc-modified-id=\"Layers-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Layers</a></span></li><li><span><a href=\"#Weights\" data-toc-modified-id=\"Weights-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Weights</a></span></li><li><span><a href=\"#Activation-Functions\" data-toc-modified-id=\"Activation-Functions-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Activation Functions</a></span></li><li><span><a href=\"#Other-Hyperparameters\" data-toc-modified-id=\"Other-Hyperparameters-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Other Hyperparameters</a></span></li></ul></li><li><span><a href=\"#Training-a-Neural-Network\" data-toc-modified-id=\"Training-a-Neural-Network-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Training a Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Backpropagation</a></span></li></ul></li></ul></li><li><span><a href=\"#Bring-in-more-complexity!\" data-toc-modified-id=\"Bring-in-more-complexity!-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bring in more complexity!</a></span><ul class=\"toc-item\"><li><span><a href=\"#ðŸ§ -Knowledge-Check:-Why-not-more-complex-all-the-time?\" data-toc-modified-id=\"ðŸ§ -Knowledge-Check:-Why-not-more-complex-all-the-time?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>ðŸ§  Knowledge Check: Why not more complex all the time?</a></span></li></ul></li><li><span><a href=\"#Let's-see-it-in-action!\" data-toc-modified-id=\"Let's-see-it-in-action!-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Let's see it in action!</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T17:49:01.027667Z",
     "start_time": "2019-12-05T17:49:00.607445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some initial setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Wait, Wait, Wait... Why a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You really should take a second to realize what tools we already have and ask yourself, \"Do we really need to use this 'neural network' if we already have so many machine learning algorithms?\"\n",
    "\n",
    "And in short, we don't need to default to a neural network but they have advantages in solving very complex problems. It might help to know that idea of neural networks was developed back in the 1950s (perceptron network). It wasn't until we had a lot of data and computational power where they became reasonably useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Starting with a Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/1600/0*No3vRruq7Dd4sxdn.png' width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice the similarity to a linear regression:\n",
    "\n",
    "\n",
    "$$ x_1 w_1 + x_2 w_2  + x_3 w_3 = \\text{output}$$\n",
    "$$ XW = \\text{output}$$\n",
    "\n",
    "But.. the MLP is too simple. We can only have binary inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Neural Networks Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/neural_network_mathematics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Parts of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Input Layer**: the initial parameters (these will be the parts we feed to our network)\n",
    "- **Output Layer**: the classification (or regression predictions)\n",
    "- **Hidden Layer(s)**: the other neurons potentially in a neural network to find more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The weights from our inputs are describing how much they should contribute to the next neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Let's add weights to our seafood restaurant scenario. Does this output help us make a better decision? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Feed Forward Network \n",
    "**Thinking in the more mathematical way, allows us to use our linear algebra knowledge**\n",
    "![](img/neural_network_mathematics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Activation Functions\n",
    "Let's discuss what kind of activation functions we have and what we can do with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return 1/(1+np.square(x))\n",
    "    return np.arctan(x)\n",
    "\n",
    "z = np.arange(-10,10,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "Range: $(0,1)$\n",
    "\n",
    "Function: $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "### Advantages\n",
    "- Relatively intuitive at classifications\n",
    "- Commonly used\n",
    "\n",
    "### Disadvantages\n",
    "- Not as efficient at training\n",
    "- vanishing gradient problem\n",
    "\n",
    "### Code & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    if (derivative == True):\n",
    "        return f * (1 - f)\n",
    "    return f\n",
    "\n",
    "y = sigmoid(z)\n",
    "dy = sigmoid(z, derivative=True)\n",
    "plt.title(\"sigmoid\")\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.plot(z, y, 'r')\n",
    "plt.plot(z, dy, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent - Tanh\n",
    "Range: $(-1,1)$\n",
    "\n",
    "Function: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "### Advantages\n",
    "- More efficient at training than sigmoid\n",
    "- Steeper gradient\n",
    "\n",
    "### Disadvantages\n",
    "- Still suffers from the vanishing gradient problem\n",
    "\n",
    "### Code & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    f = np.tanh(x)\n",
    "    if (derivative == True):\n",
    "        return (1 - (f ** 2))\n",
    "    return np.tanh(x)\n",
    "\n",
    "y = tanh(z)\n",
    "dy = tanh(z, derivative=True)\n",
    "plt.title(\"sigmoid\")\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.plot(z, y, 'r')\n",
    "plt.plot(z, dy, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "Range: $(0,\\infty)$\n",
    "\n",
    "Function: $f(x) = \n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ x<0 \\\\\n",
    "      x, & \\text{if}\\ x\\ge 0\n",
    "    \\end{cases}$\n",
    "    \n",
    "### Advantages\n",
    "- Calculation is relatively efficient\n",
    "- Specify a more positive activation\n",
    "\n",
    "### Disadvantages\n",
    "- Zero value: longer to train\n",
    "\n",
    "### Code & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = 0\n",
    "    return f\n",
    "\n",
    "plt.title(\"ReLU\")\n",
    "y = relu(z)\n",
    "dy = relu(z, derivative=True)\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.plot(z, dy, 'b')\n",
    "plt.plot(z, y, 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "Range: $(-\\infty,\\infty)$\n",
    "\n",
    "Function: $f(x) = \n",
    "    \\begin{cases}\n",
    "      - c \\cdot x, & \\text{if}\\ x<0 \\\\\n",
    "      x, & \\text{if}\\ x\\ge 0\n",
    "    \\end{cases}\\  \\text{where}\\ c\\ \\text{is some small value (0.01)}$\n",
    "    \n",
    "### Advantages\n",
    "- Helps with training speed\n",
    "\n",
    "### Disadvantages\n",
    "- Still has to compute when x is negative\n",
    "\n",
    "### Code & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = leakage\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = x[i]* leakage\n",
    "    return f\n",
    "\n",
    "# the default leakage here is 0.05!\n",
    "y = leaky_relu(z)\n",
    "dy = leaky_relu(z, derivative=True)\n",
    "plt.axhline(color=\"gray\", linewidth=1,)\n",
    "plt.axvline(color=\"gray\", linewidth=1,)\n",
    "plt.title(\"leaky ReLU\")\n",
    "plt.xlim(-10,10)\n",
    "plt.plot(z, y, 'r')\n",
    "plt.plot(z, dy, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Other Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll talk more about this in [optimizing our neural networks](optimizations.ipynb) but some hyperparameters include:\n",
    "\n",
    "- **Learning Rate ($\\alpha$)**: how big of a step we take in gradient descent\n",
    "- **Number of epochs**: how many times we repeat this process\n",
    "- **batch-size**: how many data points we use in a single training session (1 epoch)\n",
    "\n",
    "Remember, any parameter adjusted to enhance the neural network's learning _is_ a hyperparameter (this includes the actual structure of the neural net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Imagine that our neural network doesn't do great after creating. What would you do to improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The **backpropagation** algorithm takes the idea of optimally adjusting the parameters (weights) to get a better result. \n",
    "\n",
    "We do this tuning by propogating the (average) error back through the network, with the cost function $J$ guiding us and adjusting via gradient descent.\n",
    "\n",
    "> Turn down previous neurons that give a bad result\n",
    ">\n",
    "> Turn up previous neurons that give a good result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Great video explanation of backpropogation by 3Blue1Brown (part of a full playlist): [Backpropagation calculus | Deep learning, chapter 4](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)\n",
    "\n",
    "![](images/neural_network_graph_3blue1brown.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bring in more complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But can what if the data is more complicated? Can we separate these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T17:49:01.134636Z",
     "start_time": "2019-12-05T17:49:00.628Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(40)\n",
    "y = np.random.rand(40)\n",
    "z = (x + y) < np.random.rand(40)*2\n",
    "\n",
    "plt.scatter(x,y,c=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "By adding in more parts (layers) leads us to **deep learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='img/layered-neural-net.jpg' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In fact, neural networks can (in theory) approximate any continuous function! (https://en.wikipedia.org/wiki/Universal_approximation_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## ðŸ§  Knowledge Check: Why not more complex all the time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> More complexity can increase our chances of overfitting\n",
    ">\n",
    "> More parameters mean more computation (takes longer to train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll talk about ways to tune our neural network and still attempt to avoid overfititng:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Let's see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we know the different parts, let's try it out for ourselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- [playground.tensorflow.org](https://playground.tensorflow.org): A visual playground for us to train a neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
