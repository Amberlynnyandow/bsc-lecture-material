{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions and Solutions To the Optimization Problem\n",
    "Unlike the least-squares problem for linear regression, no one has yet found a closed-form solution to the optimization problem presented by logistic regression. But even if one exists, the computation would no doubt be so complex that we'd be better off using some sort of approximation method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the cost function for linear regression: <br/><br/>\n",
    "$SSE = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i(y_i - (\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in}))^2$.\n",
    "\n",
    "This function, $SSE(\\vec{\\beta})$, is convex.\n",
    "\n",
    "If we plug in our new logistic equation for $\\hat{y}$, we get: <br/><br/>\n",
    "$SSE_{log} = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i\\left(y_i - \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in})}}\\right)\\right)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## However... \n",
    "*This* function, $SSE_{log}(\\vec{\\beta})$, is [**not** convex](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c).\n",
    "\n",
    "That means that, if we tried to use gradient descent or some other approximation method that looks for the minimum of this function, we could easily find a local rather than a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Good News is... \n",
    "We can use **log-loss** instead:\n",
    "\n",
    "$\\mathcal{L}(\\vec{y}, \\hat{\\vec{y}}) = -\\frac{1}{N}\\Sigma^N_{i=1}\\left(y_iln(\\hat{y}_i)+(1-y_i)ln(1-\\hat{y}_i)\\right)$,\n",
    "\n",
    "where $\\hat{y}_i$ is the probability that $(x_{i1}, ... , x_{in})$ belongs to **class 1**.\n",
    "\n",
    "**Additional resources on the log-loss function**:\n",
    "\n",
    "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "http://wiki.fast.ai/index.php/Log_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great.. we have a cost function for log reg but how do we intepret our outputs now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, LogisticRegression,\\\n",
    "LassoCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold,\\\n",
    "cross_val_score, cross_validate, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# For demonstrative pruposes\n",
    "from scipy.special import logit, expit\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glass identification dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass.sort_values('al', inplace=True)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types 1, 2, 3 are window glass\n",
    "# types 5, 6, 7 are household glass\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the class predictions\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "logreg.fit(X, y)\n",
    "glass['household_pred_class'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the coefficients of a logistic regression? For a linear regression, the situaton was like this:\n",
    "\n",
    "- Linear Regression: We construct the best-fit line and get a set of coefficients. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change in $y$.\n",
    "\n",
    "- Logistic Regression: We find the coefficients of the best-fit line by some approximation method. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change (not in $y$ but) in $ln\\left(\\frac{y}{1-y}\\right)$.\n",
    "\n",
    "We have:\n",
    "\n",
    "$\\ln\\left(\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}\\right) = \\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k$.\n",
    "\n",
    "Exponentiating both sides:\n",
    "\n",
    "$\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)} = e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k}$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right)}\\cdot e^k$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^k\\cdot\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}$\n",
    "\n",
    "That is, the odds ratio at $x_1+1$ has increased by a factor of $e^k$ relative to the odds ratio at $x_1$.\n",
    "\n",
    "For more on interpretation, see [this page](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/binary-logistic-regression/interpret-the-results/all-statistics-and-graphs/coefficients/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the intercept\n",
    "\n",
    "logodds = logreg.intercept_\n",
    "logodds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Interpretation:** For an 'al' value of 0, the log-odds of 'household' is -6.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds = np.exp(logodds)\n",
    "odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Logistic Regression Walk-thru "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] = df['country'].replace(' ?',np.nan)\n",
    "df['workclass'] = df['workclass'].replace(' ?',np.nan)\n",
    "df['occupation'] = df['occupation'].replace(' ?',np.nan)\n",
    "\n",
    "df.dropna(how='any',inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_map = {' <=50K':1, ' >50K':0}\n",
    "df['salary'] = df['salary'].map(salary_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at a countplot to visualize our new dependent variable \n",
    "sns.countplot(x='salary', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['sex'], hue=df['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train/test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['salary'], axis=1)\n",
    "y = df['salary']\n",
    "\n",
    "split_size = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=split_size,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding our categorical variables \n",
    "### Training set first - need to fit and transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking in other features (category)\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "dummies = ohe.fit_transform(X_train[['workclass', 'education', 'marital-status', 'occupation',\n",
    "                                    'relationship', 'race', 'sex', 'country']])\n",
    "\n",
    "# Getting a DF\n",
    "dummies_df = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(),\n",
    "                         index=X_train.index)\n",
    "\n",
    "# What we'll feed int our model\n",
    "X_train_df = pd.concat([X_train[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "                                'hours-per-week']], dummies_df], axis=1)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding our test set - only transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the same transformation (not FIT) to match structure\n",
    "test_dummies = ohe.transform(X_test[['workclass', 'education', 'marital-status', 'occupation',\n",
    "                                    'relationship', 'race', 'sex', 'country']])\n",
    "test_df = pd.DataFrame(test_dummies.todense(), columns=ohe.get_feature_names(),\n",
    "                       index=X_test.index)\n",
    "X_test_df = pd.concat([X_test[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "                                'hours-per-week']], test_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg1 = LogisticRegression()\n",
    "lg1.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How did our training data do? \n",
    "lg1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate how well the model generalizes with cross-validation (only training data). _Remember_,  cross-validation works like this: First I'll partition my training data into $k$-many *folds*. Then I'll train a model on $k-1$ of those folds and \"test\" it on the remaining fold. I'll do this for all possible divisions of my $k$ folds into $k-1$ training folds and a single \"testing\" fold. Since there are $k\\choose 1$$=k$-many ways of doing this, I'll be building $k$-many models!\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "                X=X_train_df, \n",
    "                y=y_train,\n",
    "                estimator=lg1, \n",
    "                cv=10,\n",
    "                scoring='accuracy',\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the model performs on our test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "prediction = lg1.predict(X_test_df)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, prediction))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we actually trying to determine here? \n",
    "\n",
    "## Review: Bias-Variance tradeoff\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*oO0KYF7Z84nePqfsJ9E0WQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important idea here is that there is a *trade-off*: If we have too few data in our sample (training set), or too few predictors, we run the risk of high *bias*, i.e. an underfit model. On the other hand, if we have too many predictors (especially ones that are collinear), we run the risk of high *variance*, i.e. an overfit model.\n",
    "\n",
    "### Underfitting \n",
    "> Underfit models fail to capture all of the information in the data\n",
    "* low complexity --> high bias, low variance\n",
    "* training error: large\n",
    "* testing error: large\n",
    "\n",
    "### Overfitting \n",
    "> Overfit models fit to the noise in the data and fail to generalize\n",
    "* high complexity --> low bias, high variance\n",
    "* training error: low\n",
    "* testing error: large\n",
    "\n",
    "**We use training, validating and testing to help us understand if our model is over/underfitting:** \n",
    "Roughly:\n",
    "- Training data is for building the model;\n",
    "- Validation data is for *tweaking* the model;\n",
    "- Testing data is for evaluating the model on unseen data.\n",
    "<br/>\n",
    "\n",
    "- Think of **training** data as what you study for a test\n",
    "- Think of **validation** data is using a practice test (note sometimes called **dev**)\n",
    "- Think of **testing** data as what you use to judge the model\n",
    "    - A **holdout** set is when your test dataset is never used for training (unlike in cross-validation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If our model is over/underfitting... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class imbalance in most likely to result in underfitting... why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train_df, y_train) \n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on resampled data \n",
    "lg2 = LogisticRegression()\n",
    "lg2.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg2.score(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ugh... it got worse... why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If our model is overfitting... \n",
    "### Regularization could help! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, complex models are very flexible in the patterns that they can model but this also means that they can easily find patterns that are simply statistical flukes of one particular dataset rather than patterns reflective of the underlying data-generating process.\n",
    "\n",
    "When a model has large weights, the model is \"too confident\". This translates to a model with high variance which puts it in danger of overfitting! We need to punish large (confident) weights by contributing them to the error function\n",
    "\n",
    "**Some Types of Regularization:**\n",
    "\n",
    "1. Reducing the number of features\n",
    "2. Increasing the amount of data\n",
    "3. Popular techniques: Ridge, Lasso, Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Strategy Behind Ridge / Lasso / Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have **overly large coefficients**. \n",
    "\n",
    "Generally, overfitting models come from a result of high model variance. High model variance can be caused by:\n",
    "\n",
    "- having irrelevant or too many predictors\n",
    "- multicollinearity\n",
    "- large coefficients\n",
    "\n",
    "Regularization is about introducing a factor into our model designed to enforce the structure that the coefficients stay small, by _penalizing_ the ones that get too large.\n",
    "\n",
    "That is, we'll alter our loss function so that the goal now is not merely to minimize the difference between actual values and our model's predicted values. Rather, we'll add in a term to our loss function that represents the sizes of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso: L1 Regularization - Absolute Value\n",
    "- Tend to get sparse vectors (small weights go to 0)\n",
    "- Reduce number of weights\n",
    "- Good feature selection to pick out importance\n",
    "\n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}|w_i| \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge: L2 Regularization - Squared Value\n",
    "\n",
    "- Not sparse vectors (weights homogeneous & small)\n",
    "- Tends to give better results for training\n",
    "\n",
    "    \n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}w_i^2 \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should you use L1 and L2 Regularization? \n",
    "* **L2**\n",
    " - when you have features with high multicollinearity\n",
    " - reduce model complexity\n",
    " - features with large coefficients(high bias)\n",
    " \n",
    "* **L1**\n",
    " - when you have a lot of small coefficients\n",
    " - when you have LOTS of features\n",
    " - Feature selection \n",
    " \n",
    " ![](https://hackernoon.com/hn-images/0*Hb81qZ91t-kZg2eo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression has regularization already built in \n",
    "[Let's check it out](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could run a for loop to iterate over a list of possible C values to see which is best for our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [100, 10, 1, .1, .001]\n",
    "for c in C:\n",
    "    lg3 = LogisticRegression(penalty='l1', C=c, solver='liblinear')\n",
    "    lg3.fit(X_train_df, y_train)\n",
    "    print('C:', c)\n",
    "    print('Training accuracy:', lg3.score(X_train_df, y_train))\n",
    "    print('Test accuracy:', lg3.score(X_test_df, y_test))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Hyperparameters \n",
    "### LogisticRegression has several optional parameters that define the behavior of the model and approach: \n",
    "\n",
    "**penalty**- is a string ('l2' by default) that decides whether there is regularization and which approach to use. Other options are 'l1', 'elasticnet', and 'none'.\n",
    "\n",
    "**dual**- is a Boolean (False by default) that decides whether to use primal (when False) or dual formulation (when True).\n",
    "\n",
    "**tol**- is a floating-point number (0.0001 by default) that defines the tolerance for stopping the procedure.\n",
    "\n",
    "**C**- is a positive floating-point number (1.0 by default) that defines the relative strength of regularization. Smaller values indicate stronger regularization.\n",
    "\n",
    "**fit_intercept**- is a Boolean (True by default) that decides whether to calculate the intercept 𝑏₀ (when True) or consider it equal to zero (when False).\n",
    "\n",
    "**intercept_scaling**- is a floating-point number (1.0 by default) that defines the scaling of the intercept 𝑏₀.\n",
    "\n",
    "**class_weight**- is a dictionary, 'balanced', or None (default) that defines the weights related to each class. When None, all classes have the weight one.\n",
    "\n",
    "**random_state**- is an integer, an instance of numpy.RandomState, or None (default) that defines what pseudo-random number generator to use.\n",
    "\n",
    "**solver**- is a string ('liblinear' by default) that decides what solver to use for fitting the model. Other options are 'newton-cg', 'lbfgs', 'sag', and 'saga'.\n",
    "\n",
    "**max_iter**- is an integer (100 by default) that defines the maximum number of iterations by the solver during model fitting.\n",
    "\n",
    "**multi_class**- is a string ('ovr' by default) that decides the approach to use for handling multiple classes. Other options are 'multinomial' and 'auto'.\n",
    "\n",
    "**verbose**- is a non-negative integer (0 by default) that defines the verbosity for the 'liblinear' and 'lbfgs' solvers.\n",
    "\n",
    "**warm_start**- is a Boolean (False by default) that decides whether to reuse the previously obtained solution.\n",
    "\n",
    "**n_jobs**- is an integer or None (default) that defines the number of parallel processes to use. None usually means to use one core, while -1 means to use all available cores.\n",
    "\n",
    "**l1_ratio**- is either a floating-point number between zero and one or None (default). It defines the relative importance of the L1 part in the elastic-net regularization.\n",
    "\n",
    "#### Warning: \n",
    "**You should carefully match the solver and regularization method for several reasons:**\n",
    "\n",
    "'liblinear' solver doesn’t work without regularization. <br/>\n",
    "'newton-cg', 'sag', 'saga', and 'lbfgs' don’t support L1 regularization. <br/>\n",
    "'saga' is the only solver that supports elastic-net regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Logistic Regression \n",
    "\n",
    "Advantages of logistic regression:\n",
    "\n",
    "- Highly interpretable (if you remember how)\n",
    "- Model training and prediction are fast\n",
    "- Not many parameters to tune\n",
    "- Can perform well with a small number of observations\n",
    "- Outputs well-calibrated predicted probabilities\n",
    "\n",
    "Disadvantages of logistic regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the log-odds of the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: \n",
    "- Bias-Variance tradeoff is essential for optimizing all machine learning models \n",
    "- Training, validating, testing confirms if our model is over/underfitting \n",
    "- Classification metrics help to understand if we should prioritize precision/recall/f1 and give us specifics about where our model is going wrong\n",
    "- Hyperparameter tuning allows us to adjust our models to fit the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
