{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions and Solutions To the Optimization Problem\n",
    "Unlike the least-squares problem for linear regression, no one has yet found a closed-form solution to the optimization problem presented by logistic regression. But even if one exists, the computation would no doubt be so complex that we'd be better off using some sort of approximation method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the cost function for linear regression: <br/><br/>\n",
    "$SSE = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i(y_i - (\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in}))^2$.\n",
    "\n",
    "This function, $SSE(\\vec{\\beta})$, is convex.\n",
    "\n",
    "If we plug in our new logistic equation for $\\hat{y}$, we get: <br/><br/>\n",
    "$SSE_{log} = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i\\left(y_i - \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in})}}\\right)\\right)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## However... \n",
    "*This* function, $SSE_{log}(\\vec{\\beta})$, is [**not** convex](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c).\n",
    "\n",
    "That means that, if we tried to use gradient descent or some other approximation method that looks for the minimum of this function, we could easily find a local rather than a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Good News is... \n",
    "We can use **log-loss** instead:\n",
    "\n",
    "$\\mathcal{L}(\\vec{y}, \\hat{\\vec{y}}) = -\\frac{1}{N}\\Sigma^N_{i=1}\\left(y_iln(\\hat{y}_i)+(1-y_i)ln(1-\\hat{y}_i)\\right)$,\n",
    "\n",
    "where $\\hat{y}_i$ is the probability that $(x_{i1}, ... , x_{in})$ belongs to **class 1**.\n",
    "\n",
    "**Additional resources on the log-loss function**:\n",
    "\n",
    "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "http://wiki.fast.ai/index.php/Log_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great.. we have a cost function for log reg but how do we intepret our outputs now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, LogisticRegression,\\\n",
    "LassoCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold,\\\n",
    "cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# For demonstrative pruposes\n",
    "from scipy.special import logit, expit\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ri</th>\n",
       "      <th>na</th>\n",
       "      <th>mg</th>\n",
       "      <th>al</th>\n",
       "      <th>si</th>\n",
       "      <th>k</th>\n",
       "      <th>ca</th>\n",
       "      <th>ba</th>\n",
       "      <th>fe</th>\n",
       "      <th>glass_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.51966</td>\n",
       "      <td>14.77</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>72.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.51115</td>\n",
       "      <td>17.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>75.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.52320</td>\n",
       "      <td>13.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>71.75</td>\n",
       "      <td>0.09</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ri     na    mg    al     si     k     ca   ba    fe  glass_type\n",
       "id                                                                        \n",
       "22   1.51966  14.77  3.75  0.29  72.02  0.03   9.00  0.0  0.00           1\n",
       "185  1.51115  17.38  0.00  0.34  75.41  0.00   6.65  0.0  0.00           6\n",
       "40   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1\n",
       "39   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1\n",
       "51   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.0  0.16           1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glass identification dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass.sort_values('al', inplace=True)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ri</th>\n",
       "      <th>na</th>\n",
       "      <th>mg</th>\n",
       "      <th>al</th>\n",
       "      <th>si</th>\n",
       "      <th>k</th>\n",
       "      <th>ca</th>\n",
       "      <th>ba</th>\n",
       "      <th>fe</th>\n",
       "      <th>glass_type</th>\n",
       "      <th>household</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.51966</td>\n",
       "      <td>14.77</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>72.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.51115</td>\n",
       "      <td>17.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>75.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.52320</td>\n",
       "      <td>13.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>71.75</td>\n",
       "      <td>0.09</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ri     na    mg    al     si     k     ca   ba    fe  glass_type  \\\n",
       "id                                                                           \n",
       "22   1.51966  14.77  3.75  0.29  72.02  0.03   9.00  0.0  0.00           1   \n",
       "185  1.51115  17.38  0.00  0.34  75.41  0.00   6.65  0.0  0.00           6   \n",
       "40   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1   \n",
       "39   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1   \n",
       "51   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.0  0.16           1   \n",
       "\n",
       "     household  \n",
       "id              \n",
       "22           0  \n",
       "185          1  \n",
       "40           0  \n",
       "39           0  \n",
       "51           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# types 1, 2, 3 are window glass\n",
    "# types 5, 6, 7 are household glass\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the class predictions\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "logreg.fit(X, y)\n",
    "glass['household_pred_class'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.11517927]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the coefficients of a logistic regression? For a linear regression, the situaton was like this:\n",
    "\n",
    "- Linear Regression: We construct the best-fit line and get a set of coefficients. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change in $y$.\n",
    "\n",
    "- Logistic Regression: We find the coefficients of the best-fit line by some approximation method. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change (not in $y$ but) in $ln\\left(\\frac{y}{1-y}\\right)$.\n",
    "\n",
    "We have:\n",
    "\n",
    "$\\ln\\left(\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}\\right) = \\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k$.\n",
    "\n",
    "Exponentiating both sides:\n",
    "\n",
    "$\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)} = e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k}$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right)}\\cdot e^k$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^k\\cdot\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}$\n",
    "\n",
    "That is, the odds ratio at $x_1+1$ has increased by a factor of $e^k$ relative to the odds ratio at $x_1$.\n",
    "\n",
    "For more on interpretation, see [this page](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/binary-logistic-regression/interpret-the-results/all-statistics-and-graphs/coefficients/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.00934605])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the intercept\n",
    "\n",
    "logodds = logreg.intercept_\n",
    "logodds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Interpretation:** For an 'al' value of 0, the log-odds of 'household' is -6.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00245569])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds = np.exp(logodds)\n",
    "odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00244968])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = odds / (1 + odds)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Logistic Regression Walk-thru "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week         country  salary  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30162 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             30162 non-null  int64 \n",
      " 1   workclass       30162 non-null  object\n",
      " 2   fnlwgt          30162 non-null  int64 \n",
      " 3   education       30162 non-null  object\n",
      " 4   education-num   30162 non-null  int64 \n",
      " 5   marital-status  30162 non-null  object\n",
      " 6   occupation      30162 non-null  object\n",
      " 7   relationship    30162 non-null  object\n",
      " 8   race            30162 non-null  object\n",
      " 9   sex             30162 non-null  object\n",
      " 10  capital-gain    30162 non-null  int64 \n",
      " 11  capital-loss    30162 non-null  int64 \n",
      " 12  hours-per-week  30162 non-null  int64 \n",
      " 13  country         30162 non-null  object\n",
      " 14  salary          30162 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df['country'] = df['country'].replace(' ?',np.nan)\n",
    "df['workclass'] = df['workclass'].replace(' ?',np.nan)\n",
    "df['occupation'] = df['occupation'].replace(' ?',np.nan)\n",
    "\n",
    "df.dropna(how='any',inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_map = {' <=50K':1, ' >50K':0}\n",
    "df['salary'] = df['salary'].map(salary_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='salary', ylabel='count'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's look at a countplot to visualize our new dependent variable \n",
    "sns.countplot(x='salary', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amberyandow/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sex', ylabel='count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['sex'], hue=df['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train/test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['salary'], axis=1)\n",
    "y = df['salary']\n",
    "\n",
    "split_size = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=split_size,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding our categorical variables \n",
    "### Training set first - need to fit and transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>x0_ Local-gov</th>\n",
       "      <th>x0_ Private</th>\n",
       "      <th>x0_ Self-emp-inc</th>\n",
       "      <th>x0_ Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>x7_ Portugal</th>\n",
       "      <th>x7_ Puerto-Rico</th>\n",
       "      <th>x7_ Scotland</th>\n",
       "      <th>x7_ South</th>\n",
       "      <th>x7_ Taiwan</th>\n",
       "      <th>x7_ Thailand</th>\n",
       "      <th>x7_ Trinadad&amp;Tobago</th>\n",
       "      <th>x7_ United-States</th>\n",
       "      <th>x7_ Vietnam</th>\n",
       "      <th>x7_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15977</th>\n",
       "      <td>44</td>\n",
       "      <td>222978</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1504</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13574</th>\n",
       "      <td>32</td>\n",
       "      <td>83253</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23210</th>\n",
       "      <td>30</td>\n",
       "      <td>327825</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9284</th>\n",
       "      <td>23</td>\n",
       "      <td>109952</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>54</td>\n",
       "      <td>204325</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "15977   44  222978              9             0          1504              40   \n",
       "13574   32   83253             10             0             0              60   \n",
       "23210   30  327825              9             0             0              32   \n",
       "9284    23  109952             10             0             0              60   \n",
       "8841    54  204325             10             0             0              52   \n",
       "\n",
       "       x0_ Local-gov  x0_ Private  x0_ Self-emp-inc  x0_ Self-emp-not-inc  \\\n",
       "15977            0.0          1.0               0.0                   0.0   \n",
       "13574            0.0          1.0               0.0                   0.0   \n",
       "23210            1.0          0.0               0.0                   0.0   \n",
       "9284             0.0          1.0               0.0                   0.0   \n",
       "8841             1.0          0.0               0.0                   0.0   \n",
       "\n",
       "       ...  x7_ Portugal  x7_ Puerto-Rico  x7_ Scotland  x7_ South  \\\n",
       "15977  ...           0.0              0.0           0.0        0.0   \n",
       "13574  ...           0.0              0.0           0.0        0.0   \n",
       "23210  ...           0.0              0.0           0.0        0.0   \n",
       "9284   ...           0.0              0.0           0.0        0.0   \n",
       "8841   ...           0.0              0.0           0.0        0.0   \n",
       "\n",
       "       x7_ Taiwan  x7_ Thailand  x7_ Trinadad&Tobago  x7_ United-States  \\\n",
       "15977         0.0           0.0                  0.0                1.0   \n",
       "13574         0.0           0.0                  0.0                1.0   \n",
       "23210         0.0           0.0                  0.0                1.0   \n",
       "9284          0.0           0.0                  0.0                1.0   \n",
       "8841          0.0           0.0                  0.0                1.0   \n",
       "\n",
       "       x7_ Vietnam  x7_ Yugoslavia  \n",
       "15977          0.0             0.0  \n",
       "13574          0.0             0.0  \n",
       "23210          0.0             0.0  \n",
       "9284           0.0             0.0  \n",
       "8841           0.0             0.0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking in other features (category)\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "dummies = ohe.fit_transform(X_train[['workclass', 'education', 'marital-status', 'occupation',\n",
    "                                    'relationship', 'race', 'sex', 'country']])\n",
    "\n",
    "# Getting a DF\n",
    "dummies_df = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(),\n",
    "                         index=X_train.index)\n",
    "\n",
    "# What we'll feed int our model\n",
    "X_train_df = pd.concat([X_train[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "                                'hours-per-week']], dummies_df], axis=1)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding our test set - only transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the same transformation (not FIT) to match structure\n",
    "test_dummies = ohe.transform(X_test[['workclass', 'education', 'marital-status', 'occupation',\n",
    "                                    'relationship', 'race', 'sex', 'country']])\n",
    "test_df = pd.DataFrame(test_dummies.todense(), columns=ohe.get_feature_names(),\n",
    "                       index=X_test.index)\n",
    "X_test_df = pd.concat([X_test[['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
    "                                'hours-per-week']], test_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg1 = LogisticRegression()\n",
    "lg1.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7942499881589542"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How did our training data do? \n",
    "lg1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate how well the model generalizes with cross-validation (only training data). _Remember_,  cross-validation works like this: First I'll partition my training data into $k$-many *folds*. Then I'll train a model on $k-1$ of those folds and \"test\" it on the remaining fold. I'll do this for all possible divisions of my $k$ folds into $k-1$ training folds and a single \"testing\" fold. Since there are $k\\choose 1$$=k$-many ways of doing this, I'll be building $k$-many models!\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "                X=X_train_df, \n",
    "                y=y_train,\n",
    "                estimator=lg1, \n",
    "                cv=10,\n",
    "                scoring='accuracy',\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'test_score', 'train_score'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.16150284, 0.16550589, 0.17413878, 0.20070982, 0.17020321,\n",
       "        0.17748904, 0.16900682, 0.14332819, 0.2173171 , 0.18080783]),\n",
       " 'score_time': array([0.00329232, 0.00412798, 0.00307202, 0.0032692 , 0.00351477,\n",
       "        0.00343609, 0.00340819, 0.0032239 , 0.00373483, 0.00363302]),\n",
       " 'test_score': array([0.7907197 , 0.79308712, 0.79024621, 0.7877783 , 0.79441023,\n",
       "        0.80625296, 0.79819991, 0.79725249, 0.79488394, 0.79062056]),\n",
       " 'train_score': array([0.79458976, 0.7944845 , 0.79458976, 0.79465319, 0.79417956,\n",
       "        0.79296916, 0.7938638 , 0.7938638 , 0.79323229, 0.79454794])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the model performs on our test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions\n",
    "prediction = lg1.predict(X_test_df)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:\n",
      "0.7848381036578628\n",
      "----------------------------------------\n",
      "Confusion Matrix:\n",
      "[[ 581 1704]\n",
      " [ 243 6521]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, prediction))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we actually trying to determine here? \n",
    "\n",
    "## Review: Bias-Variance tradeoff\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*oO0KYF7Z84nePqfsJ9E0WQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important idea here is that there is a *trade-off*: If we have too few data in our sample (training set), or too few predictors, we run the risk of high *bias*, i.e. an underfit model. On the other hand, if we have too many predictors (especially ones that are collinear), we run the risk of high *variance*, i.e. an overfit model.\n",
    "\n",
    "### Underfitting \n",
    "> Underfit models fail to capture all of the information in the data\n",
    "* low complexity --> high bias, low variance\n",
    "* training error: large\n",
    "* testing error: large\n",
    "\n",
    "### Overfitting \n",
    "> Overfit models fit to the noise in the data and fail to generalize\n",
    "* high complexity --> low bias, high variance\n",
    "* training error: low\n",
    "* testing error: large\n",
    "\n",
    "**We use training, validating and testing to help us understand if our model is over/underfitting:** \n",
    "Roughly:\n",
    "- Training data is for building the model;\n",
    "- Validation data is for *tweaking* the model;\n",
    "- Testing data is for evaluating the model on unseen data.\n",
    "<br/>\n",
    "\n",
    "- Think of **training** data as what you study for a test\n",
    "- Think of **validation** data is using a practice test (note sometimes called **dev**)\n",
    "- Think of **testing** data as what you use to judge the model\n",
    "    - A **holdout** set is when your test dataset is never used for training (unlike in cross-validation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If our model is over/underfitting... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class imbalance in most likely to result in underfitting... why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    15890\n",
       "0     5223\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    15890\n",
      "0    15890\n",
      "Name: salary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train_df, y_train) \n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train on resampled data \n",
    "lg2 = LogisticRegression()\n",
    "lg2.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6167715544367527"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg2.score(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ugh... it got worse... why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If our model is overfitting... \n",
    "### Regularization could help! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, complex models are very flexible in the patterns that they can model but this also means that they can easily find patterns that are simply statistical flukes of one particular dataset rather than patterns reflective of the underlying data-generating process.\n",
    "\n",
    "When a model has large weights, the model is \"too confident\". This translates to a model with high variance which puts it in danger of overfitting! We need to punish large (confident) weights by contributing them to the error function\n",
    "\n",
    "**Some Types of Regularization:**\n",
    "\n",
    "1. Reducing the number of features\n",
    "2. Increasing the amount of data\n",
    "3. Popular techniques: Ridge, Lasso, Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Strategy Behind Ridge / Lasso / Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have **overly large coefficients**. \n",
    "\n",
    "Generally, overfitting models come from a result of high model variance. High model variance can be caused by:\n",
    "\n",
    "- having irrelevant or too many predictors\n",
    "- multicollinearity\n",
    "- large coefficients\n",
    "\n",
    "Regularization is about introducing a factor into our model designed to enforce the structure that the coefficients stay small, by _penalizing_ the ones that get too large.\n",
    "\n",
    "That is, we'll alter our loss function so that the goal now is not merely to minimize the difference between actual values and our model's predicted values. Rather, we'll add in a term to our loss function that represents the sizes of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso: L1 Regularization - Absolute Value\n",
    "- Tend to get sparse vectors (small weights go to 0)\n",
    "- Reduce number of weights\n",
    "- Good feature selection to pick out importance\n",
    "\n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}|w_i| \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge: L2 Regularization - Squared Value\n",
    "\n",
    "- Not sparse vectors (weights homogeneous & small)\n",
    "- Tends to give better results for training\n",
    "\n",
    "    \n",
    "$$ J(W,b) = -\\dfrac{1}{m} \\sum^m_{i=1}\\big[\\mathcal{L}(\\hat y_i, y_i)+ \\dfrac{\\lambda}{m}w_i^2 \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should you use L1 and L2 Regularization? \n",
    "* **L2**\n",
    " - when you have features with high multicollinearity\n",
    " - reduce model complexity\n",
    " - features with large coefficients(high bias)\n",
    " \n",
    "* **L1**\n",
    " - when you have a lot of small coefficients\n",
    " - when you have LOTS of features\n",
    " - Feature selection \n",
    " \n",
    " ![](https://hackernoon.com/hn-images/0*Hb81qZ91t-kZg2eo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression has regularization already built in \n",
    "[Let's check it out](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could run a for loop to iterate over a list of possible C values to see which is best for our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 100\n",
      "Training accuracy: 0.8519395633022309\n",
      "Test accuracy: 0.8423030169079456\n",
      "\n",
      "C: 10\n",
      "Training accuracy: 0.8525079335006868\n",
      "Test accuracy: 0.8415294507680406\n",
      "\n",
      "C: 1\n",
      "Training accuracy: 0.8521763842182541\n",
      "Test accuracy: 0.8430765830478506\n",
      "\n",
      "C: 0.1\n",
      "Training accuracy: 0.8492871690427699\n",
      "Test accuracy: 0.8426345452536191\n",
      "\n",
      "C: 0.001\n",
      "Training accuracy: 0.8008809738076067\n",
      "Test accuracy: 0.7903635760857554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = [100, 10, 1, .1, .001]\n",
    "for c in C:\n",
    "    lg3 = LogisticRegression(penalty='l1', C=c, solver='liblinear')\n",
    "    lg3.fit(X_train_df, y_train)\n",
    "    print('C:', c)\n",
    "    print('Training accuracy:', lg3.score(X_train_df, y_train))\n",
    "    print('Test accuracy:', lg3.score(X_test_df, y_test))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Hyperparameters \n",
    "### LogisticRegression has several optional parameters that define the behavior of the model and approach: \n",
    "\n",
    "**penalty**- is a string ('l2' by default) that decides whether there is regularization and which approach to use. Other options are 'l1', 'elasticnet', and 'none'.\n",
    "\n",
    "**dual**- is a Boolean (False by default) that decides whether to use primal (when False) or dual formulation (when True).\n",
    "\n",
    "**tol**- is a floating-point number (0.0001 by default) that defines the tolerance for stopping the procedure.\n",
    "\n",
    "**C**- is a positive floating-point number (1.0 by default) that defines the relative strength of regularization. Smaller values indicate stronger regularization.\n",
    "\n",
    "**fit_intercept**- is a Boolean (True by default) that decides whether to calculate the intercept  (when True) or consider it equal to zero (when False).\n",
    "\n",
    "**intercept_scaling**- is a floating-point number (1.0 by default) that defines the scaling of the intercept .\n",
    "\n",
    "**class_weight**- is a dictionary, 'balanced', or None (default) that defines the weights related to each class. When None, all classes have the weight one.\n",
    "\n",
    "**random_state**- is an integer, an instance of numpy.RandomState, or None (default) that defines what pseudo-random number generator to use.\n",
    "\n",
    "**solver**- is a string ('liblinear' by default) that decides what solver to use for fitting the model. Other options are 'newton-cg', 'lbfgs', 'sag', and 'saga'.\n",
    "\n",
    "**max_iter**- is an integer (100 by default) that defines the maximum number of iterations by the solver during model fitting.\n",
    "\n",
    "**multi_class**- is a string ('ovr' by default) that decides the approach to use for handling multiple classes. Other options are 'multinomial' and 'auto'.\n",
    "\n",
    "**verbose**- is a non-negative integer (0 by default) that defines the verbosity for the 'liblinear' and 'lbfgs' solvers.\n",
    "\n",
    "**warm_start**- is a Boolean (False by default) that decides whether to reuse the previously obtained solution.\n",
    "\n",
    "**n_jobs**- is an integer or None (default) that defines the number of parallel processes to use. None usually means to use one core, while -1 means to use all available cores.\n",
    "\n",
    "**l1_ratio**- is either a floating-point number between zero and one or None (default). It defines the relative importance of the L1 part in the elastic-net regularization.\n",
    "\n",
    "#### Warning: \n",
    "**You should carefully match the solver and regularization method for several reasons:**\n",
    "\n",
    "'liblinear' solver doesnt work without regularization. <br/>\n",
    "'newton-cg', 'sag', 'saga', and 'lbfgs' dont support L1 regularization. <br/>\n",
    "'saga' is the only solver that supports elastic-net regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Logistic Regression \n",
    "\n",
    "Advantages of logistic regression:\n",
    "\n",
    "- Highly interpretable (if you remember how)\n",
    "- Model training and prediction are fast\n",
    "- Not many parameters to tune\n",
    "- Can perform well with a small number of observations\n",
    "- Outputs well-calibrated predicted probabilities\n",
    "\n",
    "Disadvantages of logistic regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the log-odds of the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: \n",
    "- Bias-Variance tradeoff is essential for optimizing all machine learning models \n",
    "- Training, validating, testing confirms if our model is overfitting \n",
    "- Classification metrics help to understand if we should prioritize precision/recall/f1 and give us specifics about where our model is going wrong\n",
    "- Hyperparameter tuning allows us to adjust our models to fit the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
