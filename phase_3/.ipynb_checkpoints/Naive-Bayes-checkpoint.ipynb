{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier \n",
    "## Objectives \n",
    "- Describe how Bayes's Theorem can be used to make predictions of a target\n",
    "- Identify the appropriate variant of Naive Bayes models for a particular business problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "    # There is also a BernoulliNB for a dataset with binary predictors\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's take a second to go through an example to get a feel for how Bayes' Theorem can help us with classification. Specifically about document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Many cans of spam](img/wall_of_spam.jpeg)\n",
    "\n",
    "> This is the classic example: detecting email spam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Problem Setup**\n",
    "\n",
    "> We get emails that can be either emails we care about (***ham*** üê∑) or emails we don't care about (***spam*** ü•´). \n",
    ">\n",
    "> We can probably look at the words in the email and get an idea of whether they are spam or not just by observing if they contain red-flag words üö©\n",
    "> \n",
    "> We won't always be right, but if we see an email that uses word(s) that are more often associated with spam, then we can feel more confident as labeling that email as spam!\n",
    "\n",
    "**Naive Bayes set-up:**\n",
    "1. Look at spam and not spam (ham) emails\n",
    "2. Identify words that suggest classification\n",
    "3. Determine probability that words occur in each classification\n",
    "4. Profit (classify new emails as \"spam\" or \"ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Assumption \n",
    "Probabilities associated with different predictors are independent of each other. \n",
    "\n",
    "$P(A,B) = P(A\\cap B) = P(A)\\ P(B)$ only if independent \n",
    "\n",
    "In practice, makes sense & is usually pretty good assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Formula\n",
    "\n",
    "Let's say the word that occurs is \"cash\":\n",
    "\n",
    "$$ P(ü•´ | \"cash\") = \\frac{P(\"cash\" | ü•´)P(ü•´)}{P(\"cash\")}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(\"cash\")$\n",
    "    * That's just the probability of finding the word \"cash\"! Frequency of the word!\n",
    "- $P(ü•´)$\n",
    "    * Well, we start with some data (_prior knowledge_). So frequency of the spam occurring!\n",
    "- $P(\"cash\" | ü•´)$\n",
    "    * How frequently \"cash\" is used in known spam emails. Count the frequency across all spam emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating That Our Email Is Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just say 2% of all emails have the word \"cash\" in them\n",
    "p_cash = 0.02\n",
    "\n",
    "# We normally would measure this from our data, but we'll take \n",
    "# it that 10% of all emails we collected were spam\n",
    "p_spam = 0.10\n",
    "\n",
    "# 12% of all spam emails have the word \"cash\"\n",
    "p_cash_given_its_spam = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam_given_cash = p_cash_given_its_spam * p_spam / p_cash\n",
    "print(f'If the email has the word \"cash\" in it, there is a \\\n",
    "{p_spam_given_cash*100}% chance the email is spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending It With Multiple Words\n",
    "> With more words, the more certain we can be if it is/isn't spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam:\n",
    "\n",
    "$$ P(ü•´\\ |\"buy\",\\ \"cash\") \\propto P(\"buy\",\\ \"cash\"|\\ ü•´)\\ P(ü•´)$$\n",
    "\n",
    "Assumption of independence comes in. Our initial assumption is proportionate to the above likelihood. \n",
    "\n",
    "But because of independence: \n",
    "    \n",
    "$$ P(\"buy\",\\ \"cash\"|\\ ü•´) = P(\"buy\"|\\ ü•´)\\ P(\"cash\"|\\ ü•´)$$ (product of relevant likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize by dividing!\n",
    "\n",
    "$$\n",
    "P(ü•´\\ |\"buy\",\\ \"cash\")  =\n",
    "    \\frac\n",
    "        {P(\"buy\"|\\ ü•´)P(\"cash\"|\\ ü•´)\\ P(ü•´)}\n",
    "        {P(\"buy\"|\\ ü•´)P(\"cash\"|\\ ü•´)\\ P(ü•´) + P(\"buy\"|\\ üê∑)P(\"cash\"|\\ üê∑)\\ P(üê∑)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Modeling Example\n",
    "## Using Bayes's Theorem for Classification\n",
    "\n",
    "Let's recall Bayes's Theorem:\n",
    "\n",
    "$\\large P(h|e) = \\frac{P(h)P(e|h)}{P(e)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does this look like a classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we have three competing hypotheses $\\{h_1, h_2, h_3\\}$ that would explain our evidence $e$.\n",
    "    - Then we could use Bayes's Theorem to calculate the posterior probabilities for each of these three:\n",
    "        - $P(h_1|e) = \\frac{P(h_1)P(e|h_1)}{P(e)}$\n",
    "        - $P(h_2|e) = \\frac{P(h_2)P(e|h_2)}{P(e)}$\n",
    "        - $P(h_3|e) = \\frac{P(h_3)P(e|h_3)}{P(e)}$\n",
    "        \n",
    "- Suppose the evidence is a collection of elephant heights.\n",
    "- Suppose each of the three hypotheses claims that the elephant whose measurements we have belongs to one of the three extant elephant species (*L. africana*, *L. cyclotis*, and *E. maximus*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case the left-hand sides of these equations represent the probability that the elephant in question belongs to a given species.\n",
    "\n",
    "If we think of the species as our target, then **this is just an ordinary classification problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the right-hand sides of the equations? **These other probabilities we can calculate from our dataset.**\n",
    "\n",
    "- The priors can simply be taken to be the percentages of the different classes in the dataset.\n",
    "- What about the likelihoods?\n",
    "    - If the relevant features are **categorical**, we can simply count the numbers of each category in the dataset. For example, if the features are whether the elephant has tusks or not, then, to calculate the likelihoods, we'll just count the tusked and non-tuksed elephants per species.\n",
    "    - If the relevant features are **numerical**, we'll have to do something else. A good way of proceeding is to rely on (presumed) underlying distributions of the data. [Here](https://medium.com/analytics-vidhya/use-naive-bayes-algorithm-for-categorical-and-numerical-data-classification-935d90ab273f) is an example of using the normal distribution to calculate likelihoods. We'll follow this idea below for our elephant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elephant Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephs = pd.read_csv('data/elephants.csv', usecols=['height (cm)',\n",
    "                                                   'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'maximus']['height (cm)'],\n",
    "            ax=ax, label='maximus')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'africana']['height (cm)'],\n",
    "            ax=ax, label='africana')\n",
    "sns.kdeplot(data=elephs[elephs['species'] == 'cyclotis']['height (cm)'],\n",
    "            ax=ax, label='cyclotis')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to make prediction of species for some new elephant whose weight we've just recorded. We'll suppose the new elephant has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ht = 263"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to calculate is the mean and standard deviation for height for each elephant species. We'll use these to calculate the relevant likelihoods.\n",
    "\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_stats = elephs[elephs['species'] == 'maximus'].describe().loc[['mean', 'std'], :]\n",
    "max_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_stats = elephs[elephs['species'] == 'cyclotis'].describe().loc[['mean', 'std'], :]\n",
    "cyc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afr_stats = elephs[elephs['species'] == 'africana'].describe().loc[['mean', 'std'], :]\n",
    "afr_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prior probability - before looking at any data - \n",
    "#what is the probability I'll get an elephant of any species? \n",
    "elephs['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the PDFs of the normal distributions with the discovered means and standard deviations to calculate likelihoods. Build normal distributions with mean and scale for each elephant species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm(loc=max_stats['height (cm)'][0],\n",
    "           scale=max_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm(loc=cyc_stats['height (cm)'][0],\n",
    "          scale=cyc_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm(loc=afr_stats['height (cm)'][0],\n",
    "          scale=afr_stats['height (cm)'][1]).pdf(263)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just calculated are (approximations of) the likelihoods, i.e.:\n",
    "\n",
    "- $P(height=263 | species=maximus) = 2.04\\%$\n",
    "- $P(height=263 | species=cyclotis) = 1.50\\%$\n",
    "- $P(height=263 | species=africana) = 0.90\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Notice that they do NOT sum to 1!) But what we'd really like to know are the posteriors. I.e. what are:\n",
    "\n",
    "- $P(species=maximus | height=263)$?\n",
    "- $P(species=cyclotis | height=263)$?\n",
    "- $P(species=africana | height=263)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have equal numbers of each species, every prior is equal to $\\frac{1}{3}$. Thus we can calculate the probability of the evidence:\n",
    "\n",
    "$P(height=263) = \\frac{1}{3}(0.0204 + 0.0150 + 0.0090) = 0.0148$ (denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And therefore calculate the posteriors using Bayes's Theorem:\n",
    "\n",
    "- $P(species=maximus | height=263) = \\frac{1}{3}\\frac{0.0204}{0.0148} = 45.9\\%$;\n",
    "- $P(species=cyclotis | height=263) = \\frac{1}{3}\\frac{0.0150}{0.0148} = 33.8\\%$;\n",
    "- $P(species=africana | height=263) = \\frac{1}{3}\\frac{0.0090}{0.0148} = 20.3\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes's Theorem shows us that the largest posterior belongs to the *maximus* species. (Note also that, since the priors are all the same, the largest posterior will necessarily belong to the species with the largest likelihood!)\n",
    "\n",
    "Therefore, the *maximus* species will be our prediction for an elephant of this height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Dimensions\n",
    "\n",
    "In fact, we also have elephant weight data available in addition to their heights. To accommodate multiple features we can make use of multivariate normal distributions. A normal distribution generalized to multiple dimensions.\n",
    "![multivariate-normal](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/440px-MultivariateNormal.png)\n",
    "\n",
    "For multiple predictors, we make the simplifying assumption that **our predictors are probablistically independent**. This will often be unrealistic, but it simplifies our calculations a great deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephants = pd.read_csv('data/elephants.csv',\n",
    "                       usecols=['height (cm)', 'weight (lbs)', 'species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elephants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximus = elephants[elephants['species'] == 'maximus']\n",
    "cyclotis = elephants[elephants['species'] == 'cyclotis']\n",
    "africana = elephants[elephants['species'] == 'africana']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our new elephant with a height of 263 cm also has a weight of 7009 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeli_max = stats.multivariate_normal(mean=maximus.mean(),\n",
    "                          cov=maximus.cov()).pdf([263, 7009])\n",
    "likeli_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeli_cyc = stats.multivariate_normal(mean=cyclotis.mean(),\n",
    "                         cov=cyclotis.cov()).pdf([263, 7009])\n",
    "likeli_cyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeli_afr = stats.multivariate_normal(mean=africana.mean(),\n",
    "                         cov=africana.cov()).pdf([263, 7009])\n",
    "likeli_afr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_max = likeli_max / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_cyc = likeli_cyc / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "post_afr = likeli_afr / sum([likeli_max, likeli_cyc, likeli_afr])\n",
    "\n",
    "print(post_max)\n",
    "print(post_cyc)\n",
    "print(post_afr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB(priors=[1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = elephants.drop('species', axis=1)\n",
    "y = elephants['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.predict_proba(np.array([263, 7009]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy \n",
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gnb, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons \n",
    "**Pros:** \n",
    "- It is not only a simple approach but also a fast and accurate method for prediction.\n",
    "- Naive Bayes has very low computation cost.\n",
    "- It can efficiently work on a large dataset.\n",
    "- It performs well in case of discrete response variable compared to the continuous variable.\n",
    "- It can be used with multiple class prediction problems.\n",
    "- It also performs well in the case of text analytics problems.\n",
    "- When the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression.\n",
    "- No hyperparameters! \n",
    "\n",
    "**Cons:**\n",
    "- Require to remove correlated features because they are voted twice in the model and it can lead to over inflating importance.\n",
    "\n",
    "- If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as ‚ÄúZero Frequency‚Äù. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace smoothing. Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier. For more info on smoothing [go here](https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
