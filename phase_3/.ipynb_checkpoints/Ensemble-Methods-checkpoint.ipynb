{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "Using multiple machine learning models. Can be of same type or completely different models all together! \n",
    "<img width=50% src='img/captain_planet.jpg'/>\n",
    "\n",
    "> \"With our powers combined...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Types of Ensembles\n",
    "We can break up the different types of ensembles into a few main types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "<img src='img/stacking.jpg' width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Models, Same Data\n",
    "- A form of averaging multiple models\n",
    "- Typically uses the same training data for every model\n",
    "- The innovation comes from the use of different kinds of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta-Classifier/Meta-Regressor\n",
    "- First, we ask several different models to make predictions about the target\n",
    "- Rather than taking a simple average or vote to determine the outcome, feed these results into a final model that makes the prediction based on the other modelsâ€™ predictions\n",
    "- If it seems like we are approaching a neural network...you are correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging \n",
    "![](img/bag_of_marbles.jpg)\n",
    "> Train weak learners, combine together into one via voting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many models naturally overfit\n",
    "- Randomization â†’ New models\n",
    "- New models overfit in different ways\n",
    "- Aggregation â†’ Smooth over different ways of overfitting to reduce variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation\n",
    "- **B**ootstrap **AGG**regating\n",
    "- Algorithm to repeat many times:\n",
    "    + Create a sample from your data\n",
    "    + Train a model (e.g. a decision tree) on that sample\n",
    "- Final model comes by averaging over those many models\n",
    "\n",
    "#### 3 Options:\n",
    "1. Train each model on random sample\n",
    "2. Choose a random set of features at each decision point\n",
    "3. Choose a path at random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "<img src=\"img/try_fail_success.jpg\" width=50%/>\n",
    "\n",
    "> New model attempts to predict where the previous model made mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Letâ€™s understand the way boosting works in the below steps.\n",
    "\n",
    "1. A subset is created from the original dataset.\n",
    "2. Initially, all data points are given equal weights.\n",
    "3. A base model is created on this subset.\n",
    "4. This model is used to make predictions on the whole dataset.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd1-e1526989432375.png)\n",
    "\n",
    "5. Errors are calculated using the actual values and predicted values.\n",
    "6. The observations which are incorrectly predicted, are given higher weights.(Here, the three misclassified blue-plus points will be given higher weights)\n",
    "7. Another model is created and predictions are made on the dataset.(This model tries to correct the errors from the previous model)\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd2-e1526989487878.png)\n",
    "\n",
    "8. Similarly, multiple models are created, each correcting the errors of the previous model.\n",
    "9. The final model (strong learner) is the weighted mean of all the models (weak learners).\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/11/boosting10-300x205.png)\n",
    "\n",
    "Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd4-e1526551014644.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating: Averaging, Bagging, and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imagine training different models (maybe with different conditions) and then having them \"vote\" on what they think is best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's prepare some data to do some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier,\\\n",
    "ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.metrics import r2_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\\\n",
    "cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/salaries.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Target to 0 or 1\n",
    "df['Target'] = df['Target'] == '>50K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['Relationship','Sex','Target'],axis=1), df['Target'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "ohe.fit(X_train.select_dtypes('object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_ohe = pd.DataFrame(ohe.transform(X_train.select_dtypes('object')),\n",
    "                                  columns=ohe.get_feature_names(),\n",
    "                                    index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_ohe = pd.DataFrame(ohe.transform(X_test.select_dtypes('object')),\n",
    "                                  columns=ohe.get_feature_names(),\n",
    "                                    index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging\n",
    "> Each model uses the same data to train and then we \"vote\" to make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validate to see performance \n",
    "scores = cross_val_score(estimator=lr, X=X_tr_ohe,\n",
    "                        y=y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_te_ohe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(3)\n",
    "\n",
    "knn.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=knn, X=X_tr_ohe,\n",
    "                y=y_train, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_te_ohe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=dt, X=X_tr_ohe,\n",
    "               y=y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_te_ohe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging the Models\n",
    "#### Building a `VotingClassifier`\n",
    "\n",
    "> Of course there's a Scikit-Learn class for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = VotingClassifier(estimators=[\n",
    "    ('lr', lr),\n",
    "    ('knn', knn),\n",
    "    ('dt', dt)])\n",
    "avg.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=avg, X=X_tr_ohe,\n",
    "               y=y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg.score(X_te_ohe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Averaging with the `VotingClassifier`\n",
    "> Even if the vote is 50-50, you'd probably side with the \"smart\" ones more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This meta-estimator is not as good as one of our base estimators, so in this case the averaging did not work very well. Realizing that the decision tree is performing better than the logistic regression and the k-nearest-neighbors model, however, we might decide to build a meta-estimator by calculating a **weighted average** of the base estimators' predictions. And we can weight, or bias, this estimator in favor of the best-performing base estimator. Suppose we weight the tree 40%, the knn model 20%, and the logistic regression 40%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_avg = VotingClassifier(estimators=[\n",
    "    ('lr', lr),\n",
    "    ('knn', knn),\n",
    "    ('dt', dt)],\n",
    "    weights=[0.4, 0.2, 0.4])\n",
    "w_avg.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(estimator=w_avg, X=X_tr_ohe,\n",
    "                        y=y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_avg.score(X_te_ohe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "A single decision tree will often overfit your training data. Let's see if we have evidence of that in the current case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a sample of your X_train and fit a decision tree to it.\n",
    "- Replace the first batch of data and repeat.\n",
    "- When you've got as many trees as you like, make use of all your individual trees' predictions to come up with some holistic prediction. \n",
    "    - (Most obviously, we could take the average of our predictions, but there are other methods we might try.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is called bagging? \n",
    "* Because we're resampling our data with replacement, we're *bootstrapping*.\n",
    "* Because we're making use of our many samples' predictions, we're *aggregating*.\n",
    "* Because we're bootstrapping and aggregating all in the same algorithm, we're *bagging*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate a BaggingRegessor\n",
    "# Note the base esimator is by default a decision tree\n",
    "bag = BaggingClassifier(n_estimators=100,\n",
    "                       verbose=1,\n",
    "                       random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "\n",
    "scores = cross_val_score(estimator=bag, X=X_tr_ohe,\n",
    "               y=y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on test\n",
    "\n",
    "bag.score(X_te_ohe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Aside Story - Bananas ðŸŒ\n",
    "\n",
    "![Many individual yellow bananas](img/bananas.jpg)\n",
    "\n",
    "Banana trees can be susceptible to [Panama's disease](https://en.wikipedia.org/wiki/Panama_disease)\n",
    "\n",
    "They're all clones!\n",
    "\n",
    "Similarly, all the Decision Trees will be the same if given the same data! (A clone!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Goods & The Bads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Goods**\n",
    "\n",
    "- Super friend! - Captain Planet\n",
    "- High performance \n",
    "    + low variance\n",
    "- Transparent - Look at each individual tree to see it's decisions \n",
    "    + inherited from Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Bads**\n",
    "\n",
    "- We got so many trees to plant...\n",
    "- Computationally expensive\n",
    "- Memory\n",
    "    + all trees stored in memory\n",
    "    + think back to k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breed a Variety of Trees\n",
    "Let's add an extra layer of randomization: Instead of using all the features of my model to optimize a branch at each node, I'll just choose a subset of my features.\n",
    "\n",
    "That's the essence of a random forest model. Note that there are now two levels of random sampling happening: To build a new tree, I'll be taking only some of my data points; and at any branching point in a tree, I'll be using only some of my features to determine the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "1. Save a portion of data for validation (**out-of-bag**), the rest for training (**bag**)\n",
    "2. The data for training (**bag**) is then split up by randomly selecting predictors\n",
    "3. Grow/train your tree with the training data using just those features\n",
    "4. Use our validation set (**out-of-bag**), take out the columns used in our tree from the previous step, and predict using the tree & this *out-of-bag* data\n",
    "5. Compare on how well the tree did *out-of-bag error*\n",
    "6. Repeat to make new trees and use the result to \"vote\" for the final decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the Example Data\n",
    "\n",
    "> Here's the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) on `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a RandomForestClassifier\n",
    "\n",
    "rfr = RandomForestClassifier(max_features='sqrt',\n",
    "                            max_samples=0.5,\n",
    "                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n",
    "\n",
    "rfr.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "\n",
    "scores = cross_val_score(estimator=rfr, X=X_tr_ohe,\n",
    "               y=y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on test\n",
    "\n",
    "score = rfr.score(X_te_ohe, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool Features of Random Forests\n",
    "There are some extra investigations we can do with random forests since they're built of decision trees.\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> Not all of these are _specific_ to random forests and can be applied to other (ensemble) models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate Your Forest ðŸŒ²ðŸŒ²ðŸ‘€ðŸŒ²ðŸŒ²\n",
    "We can check out our trained estimators after training the ensemble. This isn't necessarily unique to random forests, but since the base model is always a decision tree we can really investigate how the model is working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_estimators = rfr.estimators_ \n",
    "print(len(model_estimators))\n",
    "model_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall model\\'s score was {score:.3f}')\n",
    "print('='*70)\n",
    "\n",
    "for model in model_estimators[:5]:\n",
    "    display(model)\n",
    "    model_score = model.score(X_te_ohe, y_test)\n",
    "    print(f'\\tModel gave score of {model_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "We can use [`.feature_importances_`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_) property of the trained model to get an idea of what features mattered the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import = {name: score \n",
    "                   for name, score \n",
    "                       in zip(X_te_ohe.columns, rfr.feature_importances_)\n",
    "}\n",
    "feat_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees (Extra Trees)\n",
    "\n",
    "Sometimes we might want even one more bit of randomization. Instead of always choosing the *optimal* branching path, we might just choose a branching path at random. If we're doing that, then we've got extremely randomized trees.\n",
    "\n",
    "There are now **three** levels of randomization: sampling of data, sampling of features, and random selection of branching paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an ExtraTreesRegressor\n",
    "\n",
    "etr = ExtraTreesClassifier(max_features='sqrt',\n",
    "                         max_samples=0.5,\n",
    "                         bootstrap=True,\n",
    "                         random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n",
    "\n",
    "etr.fit(X_tr_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "\n",
    "scores = cross_val_score(estimator=etr, X=X_tr_ohe,\n",
    "               y=y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on test\n",
    "\n",
    "etr.score(X_te_ohe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Remember weighted averaging? Stacking is about using DS models to estimate those weights for us. This means we'll have one layer of base estimators and another layer that is \"**trained to optimally combine the model predictions to form a new set of predictions**\". See [this short blog post](https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import os\n",
    "wb = xlrd.open_workbook('data/Sales Report.xls',\n",
    "                        logfile=open(os.devnull, 'w'))\n",
    "\n",
    "sales = pd.read_excel(wb)\n",
    "sales = sales.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Sub-Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = sales[['Discount', 'Profit']].columns\n",
    "X_cat = sales[['Category', 'Sub-Category']].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sales[['Discount', 'Profit',\n",
    "          'Category', 'Sub-Category']]\n",
    "y = sales['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrans = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "catTrans = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(drop='first',\n",
    "                          sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = ColumnTransformer(transformers=[\n",
    "    ('num', numTrans, X_num),\n",
    "    ('cat', catTrans, X_cat)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pp = pp.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('lr', LinearRegression()),\n",
    "    ('knn', KNeighborsRegressor()),\n",
    "    ('dt', DecisionTreeRegressor())\n",
    "]\n",
    "\n",
    "sr = StackingRegressor(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.fit(X_tr_pp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pp = pp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.score(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Base Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_tr_pp, y_train)\n",
    "lr.score(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor().fit(X_tr_pp, y_train)\n",
    "knn.score(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = DecisionTreeRegressor().fit(X_tr_pp, y_train)\n",
    "rt.score(X_test_pp, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of Random forests \n",
    "**Pros:**\n",
    "* Strong performance because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well.\n",
    "\n",
    "* Interpretability: each tree in the random forest is a Glass-Box Model (meaning that the model is interpretable, allowing us to see how it arrived at a certain decision), the overall random forest is, as well!\n",
    "\n",
    "**Cons:**\n",
    "* Computational complexity: On large datasets, the runtime can be quite slow compared to other algorithms.\n",
    "\n",
    "* Memory usage: Random forests tend to have a larger memory footprint that other models. It's not uncommon to see random forests that were trained on large datasets have memory footprints in the tens, or even hundreds of MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
