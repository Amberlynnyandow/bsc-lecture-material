{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "## Objectives \n",
    "- Describe the decision tree modeling algorithm\n",
    "    - Specifically, understand Entropy and Gini Impurity\n",
    "- Hyper-parameter tune to build the best tree structure\n",
    "- Interpret the feature importances of a fitted model\n",
    "- Explain the pros and cons of decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What exactly is a decision tree? \n",
    "> **Decision trees** are a supervised learning model that makes use of past data to form a graph/pathway which leads to the model making _decisions_ on it's predictions. Every time we make a decision, we split up, or _partition_, the data based on the features.\n",
    "\n",
    "That explanation should now lead us to the question... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how does it determine the 'best' split? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**First: Let's Play 20 Questions** \n",
    "![](https://assets.rebelmouse.io/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vbWVkaWEucmJsLm1zL2ltYWdlP3U9JTJGZmlsZXMlMkYyMDE3JTJGMDElMkYwOSUyRjYzNjE5NTI5ODA3OTc5MzMxMzE2NzA3NjMxNzlfMjAtUXVlc3Rpb25zLmpwZyZobz1odHRwcyUzQSUyRiUyRmF6NjE2NTc4LnZvLm1zZWNuZC5uZXQmcz0zJmg9YjAwZjA3NzA5N2MyY2ExNjA1MTYyNmU2NDExMzhjY2U5ZWRkODNhNjRkZGU1ZTE0OGRlYzIwMzUzNTU5YThiNSZzaXplPTk4MHgmYz0zMTI5MTM3NjcxIiwiZXhwaXJlc19hdCI6MTY3NDY3MzgzOX0.8J95RyGLZ8f5hLJtL5WbxPKxFGYemHdcLwu4tGUfaMs/img.jpg?width=1200&height=628)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <details>\n",
    "        <summary>One More Question: <br/>\n",
    "            What are some real life examples where our society uses a decision tree mentality to come to an answer? </summary>\n",
    "  - A mechanic will ask you a series of questions to understand the problem with your car <br/>\n",
    "  - A doctor <br/>\n",
    "  - A loan officer <br/>\n",
    "  - Anyone who is trying to sell you something   \n",
    "    </details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology Related to Decision Trees\n",
    "\n",
    "- **Root Node:** Represents entire population or sample.\n",
    "- **Decision Node:** Node that is split.\n",
    "- **Leaf/ Terminal Node:** Node with no children.\n",
    "- **Pruning:** Removing nodes.\n",
    "- **Branch / Sub-Tree:** A sub-section of a decision tree.\n",
    "- **Parent and Child Node:** A node divided into sub-nodes is the parent; the sub-nodes are its children.\n",
    "\n",
    "<img src='./img/decision_leaf.webp' width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to determine the best split: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy \n",
    "One way to assess the value of a split is to measure how *disordered* our groups are, and there is a notion of *entropy* that measures precisely this.\n",
    "\n",
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma^n_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group, where $n$ is the number of groups (i.e. target values).\n",
    "\n",
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "Suppose we're working on a classification algorithm designed to sort customers into two classes: those who pay their sales bills and those who don't.\n",
    "\n",
    "Each row in my dataframe represents a customer, and I have many predictors (columns) in my dataframe, including:\n",
    "\n",
    "- salary\n",
    "- total_bill\n",
    "- club_member (boolean)\n",
    "- years_post-sec_ed\n",
    "\n",
    "Let's look at a simple set of data. The 'paid' column is our target or dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "custs = pd.DataFrame([[45000, 1000, True, 2, False], [70000, 100, True, 10, True],\n",
    "             [30000, 2000, False, 0, False], [90000, 500, True, 2, True],\n",
    "             [70000, 200, True, 5, False]],\n",
    "            columns=['salary', 'total_bill', 'club_member', 'years_post-sec_ed',\n",
    "                    'paid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "I partition my data by asking a question about the independent variables. The goal is to ask the right questions in the right order so that the resultant groups are \"pure\" with respect to the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = custs.sort_values(['salary'])\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the customer a club member? <br/>\n",
    "This would divide my data into two groups:\n",
    "\n",
    "- Group 1: <br/>\n",
    "data points: 0, 1, 3, 4 (dep-var.: False, True, True, False)\n",
    "\n",
    "- Group 2: <br/>\n",
    "data points: 2 (dep-var.: False)\n",
    "\n",
    "While I've isolated one of the customers who haven't paid in the second group, the first group is an even mix of payers and non-payers. So this split is not particularly good.\n",
    "\n",
    "Would a different question split our data more effectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the customer's salary less than $60k? <br/>\n",
    "This would divide my data into two groups:\n",
    "\n",
    "- Group 1: <br/>\n",
    "data points: 0, 2 (dep. var.: False, False)\n",
    "\n",
    "- Group 2: <br/>\n",
    "data points: 1, 3, 4 (dep. var.: True, True, False)\n",
    "\n",
    "This does a better job of partitioning my data according to the values of the dependent variable: The first group contains only customers who have not paid their bills, and the second group contains only one customer who has not paid her bill.\n",
    "\n",
    "So a (very simple!) model that predicts: (i) that customers who make less than 60k *won't* pay their bill, and (ii) that customers who make 60k or more will pay their bill\n",
    "\n",
    "would perform fairly well.\n",
    "\n",
    "But how would my partition be best split? And how do I really know that the second split is better than the first? Can I do better than intuition here? If our goal is to have classes fully ordered(1s in one group and 0s in the other) we need to measure disorder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large E = -\\Sigma^n_i p_i\\log_2(p_i)$\n",
    "\n",
    "In the present case we have only two groups of interest: the payers (2/5) and the non-payers (3/5).\n",
    "\n",
    "So our entropy for this toy dataset is:\n",
    "\n",
    "$-0.4*\\log_2(0.4) -0.6*\\log_2(0.6)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "ent_whole = -.4*log(.4,2)-0.6*log(.6,2)\n",
    "ent_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty disordered!\n",
    "\n",
    "To calculate the entropy of a split, we're going to want to calculate the entropy of each of the groups made by the split, and then calculate a weighted average of those groups' entropies––weighted, that is, by the size of the groups. Let's calculate the entropy of the split produced by our question above about salary:\n",
    "\n",
    "Group 1:\n",
    "\n",
    "$E_{g1} = 0 * \\log_2(0) - 1 * \\log_2(1) = 0$. This is a pure group! The probability of being a payer in Group 1 is 0 and the probability of being a non-payer in Group 1 is 1.\n",
    "\n",
    "Group 2:\n",
    "\n",
    "$E_{g2} = \\frac{2}{3} * \\log_2\\left(\\frac{2}{3}\\right) - \\frac{1}{3} * \\log_2\\left(\\frac{1}{3}\\right)$.\n",
    "\n",
    "Once again, using math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_grp2 = -(2/3)*log(2/3,2)-1/3*log(1/3,2)\n",
    "ent_grp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the whole entropy for this split, we'll do a weighted sum of the two group entropies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2/5)*0 + (3/5)*ent_grp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given split, the information gain is simply the entropy of the parent group less the entropy of the split.\n",
    "\n",
    "For a given parent, then, we maximize our model's performance by minimizing the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "to look at the entropies of all possible splits, and\n",
    "to choose the split with the lowest entropy.\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity\n",
    "\n",
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_ip_i^2$, or, equivalently, $\\large G = \\Sigma_ip_i(1-p_i)$.\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group.\n",
    "\n",
    "**Gini Impurity will always be between 0 and 0.5. The closer to 0.5, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "df = pd.read_csv('data/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's calculate gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "df['pregnancies_new'] = df['Pregnancies'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying chest pain and heart disease \n",
    "preg_low_and_HD = df.loc[(df['pregnancies_new'] == 0) & (df['Outcome'] == 1)]\n",
    "preg_low_and_NHD = df.loc[(df['pregnancies_new'] == 0) & (df['Outcome'] == 0)]\n",
    "preg_high_and_HD = df.loc[(df['pregnancies_new'] == 1) & (df['Outcome'] == 1)]\n",
    "preg_high_and_NHD  = df.loc[(df['pregnancies_new'] == 1) & (df['Outcome'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preg_low_and_HD))\n",
    "print(len(preg_low_and_NHD))\n",
    "print(len(preg_high_and_HD))\n",
    "print(len(preg_high_and_NHD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets calculate gini impurity for each leaf node \n",
    "preg_low_leaf = 1 - (86/(86+263))**2 - (263/(86+263))**2\n",
    "preg_high_leaf = 1 - (182/(182+237))**2 - (237/(182+237))**2\n",
    "print(f\"{preg_low_leaf} is the gini impurity for the low pregnancies leaf\")\n",
    "print(f\"{preg_high_leaf} is the gini impurity for the high pregnancies leaf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to calculate the total gini impurity for this root node to determine if it should be used as the root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating total gini impurity for pregnancies_new\n",
    "total_gini_preg_new = ((349/768)*.3714) + ((419/768)*.4914)\n",
    "total_gini_preg_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is done for each of our features and the feature with the lowest gini impurity score will become the root node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy arrays for predictors and target variables \n",
    "X = df.drop(['Outcome', 'pregnancies_new'],axis=1).values\n",
    "y = df['Outcome'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*40)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Classification Matrix:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line of code might help if you want to double check precision/recall values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Tree \n",
    "\n",
    "Being able to visualize the splits at each step in our decision tree is an excellent way to understand what your algo is actually doing and it helps to understand the data.\n",
    "\n",
    "_NOTE:_ You will likely need to run the cell below before you can use the following code. Export_graphviz converts our classifier into a dot file and pydotplus converts the dot file in a png which can then be displayed into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!brew install graphviz\n",
    "#!pip install -U pydotplus\n",
    "\n",
    "#For installing graphviz on windows use the url below \n",
    "#https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/20971549/How+to+install+Graphviz+software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "col_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "             'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = col_names,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance with Decision Trees\n",
    "This unpruned tree is not very interpretable. The _CART algorithm_ will repeatedly partition data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable. In practice this often means that the final subsets (known as the leaves of the tree) each consist of only one or a few data points. \n",
    "\n",
    "This tends to result in low-bias, high variance models. Let's prune these trees and see if we get a better output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "![](https://miro.medium.com/max/1136/1*3MDxpY_pIMs0yb4dc55KpQ.jpeg)\n",
    "\n",
    "The recursive binary splitting procedure described above needs to know when to stop splitting as it works its way down the tree with the training data.\n",
    "\n",
    "**min_samples_leaf:**  The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.\n",
    "\n",
    "**max_leaf_nodes:** \n",
    "Reduce the number of leaf nodes.\n",
    "\n",
    "**max_depth:**\n",
    "Reduce the depth of the tree to build a generalized tree.\n",
    "\n",
    "**min_impurity_split :**\n",
    "A node will split if its impurity is above the threshold, otherwise it will be a leaf.\n",
    "\n",
    "[Great Medium Article that thoroughly goes over each hyperparameter](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*40)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = col_names,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances\n",
    "The fitted tree has an attribute called `ct.feature_importances_`. What does this mean? Roughly, the importance (or \"Gini importance\") of a feature is a sort of weighted average of the impurity decrease at internal nodes that make use of the feature. The weighting comes from the number of samples that depend on the relevant nodes.\n",
    "\n",
    "> The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. See [`sklearn`'s documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats = clf.feature_importances_\n",
    "top_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fi, feature in zip(col_names, clf.feature_importances_):\n",
    "    print(fi, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "# creating list of column names\n",
    "feat_names=list(col_names)\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(top_feats)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [feat_names[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure()\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(X_train.shape[1]), top_feats[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "- The decision tree is a \"white-box\" type of ML algorithm. It shares internal decision-making logic, which is not available in the black-box type of algorithms such as Neural Network.\n",
    "\n",
    "- Its training time is faster compared to other algorithms such as neural networks.\n",
    "\n",
    "- The decision tree is a non-parametric method, which does not depend upon probability distribution assumptions.\n",
    "\n",
    "- Decision trees can handle high-dimensional data with good accuracy.\n",
    "\n",
    "### Pros\n",
    "- East to interpret\n",
    "- Easily capture non-linear patterns\n",
    "- No need to normalize columns\n",
    "- feature engineering/importance\n",
    "- Non-parametric/no assumptions\n",
    "\n",
    "### Cons\n",
    "- Sensitive to noisy data(tends to overfit), can be reduced with tuning\n",
    "- Sensitive to variance, can be reduced by bagging/boosting\n",
    "- Biased when you have imbalanced data(can be fixed with smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
