{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "> **Pipelines** can keep our code neat and clean all the way from gathering & cleaning our data, to creating models & fine-tuning them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduces Complexity\n",
    "> You can focus on parts of the pipeline at a time and debug or adjust parts as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenient\n",
    "> You can summarize your fine-detail steps into the pipeline. That way you can focus on the big-picture aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexible\n",
    "\n",
    "> You can also use pipelines to be applied to different models and can perform optimization techniques like grid search and random search on hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Using `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as stats\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\\\n",
    "cross_val_score, RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting some data\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without the Pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformers (will adjust/massage the data)\n",
    "imputer = SimpleImputer(strategy=\"median\") # replaces missing values\n",
    "std_scaler = StandardScaler() # scales the data\n",
    "\n",
    "# Define the classifier (predictor) to train\n",
    "rf_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Have the classifer (and full pipeline) learn/train/fit from the data\n",
    "X_train_filled = imputer.fit_transform(X_train)\n",
    "X_train_scaled = std_scaler.fit_transform(X_train_filled)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict using the trained classifier (still need to do the transformations)\n",
    "X_test_filled = imputer.transform(X_test)\n",
    "X_test_scaled = std_scaler.transform(X_test_filled)\n",
    "y_pred = rf_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that if we were to add more steps in this process, we'd have to change both the *training* and *testing* processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `Pipeline` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('rf_clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "# Train the pipeline (tranformations & predictor)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline (includes the transfomers & trained predictor)\n",
    "predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we need to change our process, we change it _just once_ in the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searching a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset('penguins')\n",
    "penguins = penguins.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = penguins.pop('species')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    penguins, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nums = X_train.select_dtypes('float64')\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(X_train_nums)\n",
    "nums_df = pd.DataFrame(ss.transform(X_train_nums),\n",
    "                      index=X_train_nums.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = X_train.select_dtypes('object')\n",
    "\n",
    "ohe = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse=False)\n",
    "\n",
    "dums = ohe.fit_transform(X_train_cat)\n",
    "dums_df = pd.DataFrame(dums,\n",
    "                       columns=ohe.get_feature_names(),\n",
    "                       index=X_train_cat.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Intermediary step to treat categorical and numerical data differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "                \n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(drop='first',\n",
    "                         sparse=False))\n",
    "])\n",
    "\n",
    "trans = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_pipeline, X_train_nums.columns),\n",
    "    ('categorical', categorical_pipeline, X_train_cat.columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally showing we can fit the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Performing grid search on the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_grid = {'knn__n_neighbors': [3, 5, 7], 'knn__p': [1, 2, 3]}\n",
    "\n",
    "gs_pipe = GridSearchCV(estimator=model_pipe, param_grid=pipe_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs_pipe.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pipe.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Data Leakage\n",
    "Note we still have to be careful in performing a grid search!\n",
    "\n",
    "We can accidentally \"leak\" information by doing transformations with the whole data set, instead of just the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of leaking information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# Scales over all of the X-train data! (validation set will be considered in scaling)\n",
    "scaled_data = scaler.fit_transform(X_train.select_dtypes('float64'))\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': [1, 3, 5],\n",
    "    'metric': ['minkowski', 'manhattan'],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "clf_dt = KNeighborsClassifier()\n",
    "clf = GridSearchCV(clf_dt, parameters)\n",
    "clf.fit(X_train.select_dtypes('float64'), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Grid Search with no leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Note you use the part of the pipeline's name `NAME__{parameter}`\n",
    "parameters = {\n",
    "    'scaler__with_mean': [True, False],\n",
    "    'clf__n_neighbors': [1, 3, 5],\n",
    "    'clf__metric': ['minkowski', 'manhattan'],\n",
    "    'clf__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "cv.fit(X_train.select_dtypes('float64'), y_train)\n",
    "y_pred = cv.predict(X_test.select_dtypes('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
