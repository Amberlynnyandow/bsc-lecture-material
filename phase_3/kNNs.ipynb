{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest-Neighbors\n",
    "![wilson](img/wilson.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix,\\\n",
    "recall_score, precision_score, accuracy_score\n",
    "from src.confusion import plot_confusion_matrix\n",
    "from src.k_classify import predict_one\n",
    "from src.plot_train import *\n",
    "from src.euclid import *\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "- Describe the $k$-nearest neighbors algorithm\n",
    "- Identify multiple common distance metrics\n",
    "- Tune $k$ appropriately in response to models with high bias or variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept of the $k$-Nearest Neighbors Algorithm\n",
    "> In **supervised learning** we use example data (_training data_) to inform our predictions of future data. \n",
    "One strategy to make predictions on a new data is to just look at what similar data points are like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/best_k_fs.png)\n",
    "We can say _nearby_ points are _similar_ to one another. There are a few different wasy to determine how \"close\" data points are to one another. \n",
    "![](https://miro.medium.com/max/1400/1*FlMiuoENrq52tMV4S6LSZg.png)\n",
    "\n",
    "* Euclidean and Manhattan distance is best used for continuous variables.<br/> \n",
    "Check out the [Level Up section on distance metrics](#Level-Up:-Distance-Metrics) for some more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of $k$NN\n",
    "![](img/knn-process.png)\n",
    "\n",
    "**The algorithm can be summarized as:**\n",
    "1. A positive integer k is specified, along with a new sample\n",
    "2. We select the k entries in our database which are closest to the new sample\n",
    "3. We find the most common classification of these entries(voting)\n",
    "4. This is the classification we give to the new sample\n",
    "\n",
    "**A few other features of KNN:**\n",
    "* KNN stores the entire training dataset which it uses as its representation.\n",
    "* KNN does not learn any model parameters.\n",
    "* KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance.\n",
    "\n",
    "**Note:** KNN performs better with a low number of features. The more features you have the more data you need. You increase the dimensions everytime you add another feature. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "display(iris)\n",
    "# Let's convert this over to NumPy array\n",
    "X = iris.iloc[:,:2].to_numpy()\n",
    "# Let's convert classes to numerical values\n",
    "y = LabelEncoder().fit_transform(iris['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], ax=ax, hue=y, palette='colorblind')\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "neigh.fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Some Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made up data points\n",
    "pred_pts = np.array([\n",
    "    [7.0, 3.0],\n",
    "    [8.0, 3.5],\n",
    "    [7.0, 4.0],    \n",
    "    [4.0, 3.0],\n",
    "    [5.0, 3.0],\n",
    "    [5.5, 4.0],\n",
    "    [5.0, 2.0],\n",
    "    [6.0, 2.5],\n",
    "    [5.8, 3.5],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see these new points against the training data. Think about how they'll be made classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], ax=ax, hue=y, palette='colorblind')\n",
    "sns.scatterplot(x=pred_pts[:,0], ax=ax, y=pred_pts[:,1], marker=\"*\", s=200, edgecolor='black', color='magenta')\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_y = neigh.predict(pred_pts)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities for KNN (how they voted)\n",
    "for p,prob in zip(pred_y,neigh.predict_proba(pred_pts)):\n",
    "    print(f'{p}: {prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see those predictions plotted with the other points after the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "sns.scatterplot(x=X[:,0],y=X[:,1], ax=ax, hue=y, palette='colorblind')\n",
    "sns.scatterplot(x=pred_pts[:,0], ax=ax, y=pred_pts[:,1], hue=pred_y, palette='colorblind', marker=\"*\", s=200, edgecolor='black')\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Pros and Cons\n",
    "\n",
    "## Pros:\n",
    "- Lazy learning (no training phase)\n",
    "- No assumptions about data — useful, for example, for nonlinear data\n",
    "- Simple algorithm — to explain and understand/interpret\n",
    "- High accuracy (relatively) — it is pretty high but not competitive in comparison to better supervised learning models\n",
    "- Versatile — useful for classification or regression\n",
    "\n",
    "## Cons: \n",
    "- Computationally expensive — because the algorithm stores all of the training data. High memory requirement\n",
    "- Prediction stage might be slow with large datasets\n",
    "- Sensitive to irrelevant features and the scale of the data\n",
    "- Not robust; doesn't generalize well\n",
    "- \"Curse of Dimensionality\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Classify with the Titanic Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/cleaned_titanic.csv')\n",
    "titanic = titanic.iloc[:, :-2]\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For visualization purposes, we will use only two features for our first model. This dataset presents a binary classification problem, with our target being the `Survived` feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['Age', 'Fare']]\n",
    "y = titanic['Survived']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Split \n",
    "**We've only seen this in the context of cross-validation but we can split our training data again into a training/validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          random_state=42,\n",
    "                                          test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_t, y_t)\n",
    "print(f\"training accuracy: {knn.score(X_t, y_t)}\")\n",
    "print(f\"validation accuracy: {knn.score(X_val, y_val)}\")\n",
    "\n",
    "y_hat = knn.predict(X_val)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_val, y_hat), classes=['Perished', 'Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$-NN algorithm works by simply storing the training set in memory, then measuring the distance from the training points to a new point.\n",
    "\n",
    "Let's view our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_viz = X_t.sample(15, random_state=40)\n",
    "y_for_viz = y_t[X_for_viz.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'], \n",
    "                hue=y_for_viz, palette={0: 'red', 1: 'green'}, \n",
    "                s=200, ax=ax)\n",
    "\n",
    "ax.set_xlim(0, 80)\n",
    "ax.set_ylim(0, 80)\n",
    "plt.legend()\n",
    "plt.title('Subsample of Training Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_viz = X_t.sample(15, random_state=40)\n",
    "y_for_viz = y_t[X_for_viz.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'],\n",
    "                hue=y_for_viz, palette={0: 'red', 1: 'green'},\n",
    "                s=200, ax=ax)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#################^^^Old code^^^##############\n",
    "####################New code#################\n",
    "\n",
    "# Let's take one sample from our validation set and plot it\n",
    "new_x = pd.DataFrame(X_val.loc[484]).T\n",
    "new_y = y_val[new_x.index]\n",
    "\n",
    "sns.scatterplot(new_x['Age'], new_x['Fare'], color='blue',\n",
    "                s=200, ax=ax, label='New', marker='P')\n",
    "\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, $k$-NN finds the $k$ nearest points. $k$ corresponds to the `n_neighbors` parameter defined when we instantiate the classifier object. **If $k$ = 1, then the prediction for a point will simply be the value of the target for the nearest point.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different $k$ Values\n",
    "A big factor in this algorithm is choosing $k$\n",
    "![](img/k_vs_errors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our training data, then predict what our validation point will be based on the (one) closest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_for_viz, y_for_viz)\n",
    "knn.predict(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we raise the value of $k$, $k$-NN will act democratically: It will find the $k$ closest points, and take a vote based on the labels.**\n",
    "\n",
    "Let's raise $k$ to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3.fit(X_for_viz, y_for_viz)\n",
    "knn3.predict(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not easy to tell what which points are closest by eye.\n",
    "\n",
    "Let's update our plot to add indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_viz = X_t.sample(15, random_state=40)\n",
    "y_for_viz = y_t[X_for_viz.index]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.scatterplot(X_for_viz['Age'], X_for_viz['Fare'], hue=y_for_viz, \n",
    "                palette={0: 'red', 1: 'green'}, s=200, ax=ax)\n",
    "\n",
    "\n",
    "# Now let's take another sample\n",
    "\n",
    "# new_x = X_val.sample(1, random_state=33)\n",
    "new_x = pd.DataFrame(X_val.loc[484]).T\n",
    "new_x.columns = ['Age', 'Fare']\n",
    "new_y = y_val[new_x.index]\n",
    "\n",
    "print(new_x)\n",
    "sns.scatterplot(new_x['Age'], new_x['Fare'], color='blue', \n",
    "                s=200, ax=ax, label='New', marker='P')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 100)\n",
    "plt.legend()\n",
    "\n",
    "#################^^^Old code^^^##############\n",
    "####################New code#################\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "for index in X_for_viz.index:\n",
    "    ax.text(X_for_viz.Age[index]+0.7, X_for_viz.Fare[index],\n",
    "            s=index, horizontalalignment='left', size='medium',\n",
    "            color='black', weight='semibold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sklearn`'s NearestNeighors object to see the exact calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_viz = pd.merge(X_for_viz, y_for_viz, left_index=True, right_index=True)\n",
    "neighbor = NearestNeighbors(3)\n",
    "neighbor.fit(X_for_viz)\n",
    "nearest = neighbor.kneighbors(new_x)\n",
    "\n",
    "nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_viz.iloc[nearest[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Euclidean distance to see how close they are to this point\n",
    "print(((29-24)**2 + (33-25.4667)**2)**0.5)\n",
    "print(((26-24)**2 + (16.1-25.4667)**2)**0.5)\n",
    "print(((20-24)**2 + (15.7417-25.4667)**2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about with 5 neighbors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_for_viz, y_for_viz)\n",
    "knn.predict(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing Different $k$ Values\n",
    "Let's iterate through $k$, odd numbers 1 through 10, and see the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 10, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_for_viz, y_for_viz)\n",
    "    print(f'k={k}', knn.predict(new_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "For any distance-based algorithms, scaling is very important. Look at how the shape of the array changes before and after scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![non-normal](img/nonnormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![normal](img/normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our data_for_viz dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          random_state=42,\n",
    "                                          test_size=0.25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_ind = X_t.index\n",
    "X_col = X_t.columns\n",
    "\n",
    "X_t_s = pd.DataFrame(ss.fit_transform(X_t))\n",
    "X_t_s.index = X_ind\n",
    "X_t_s.columns = X_col\n",
    "\n",
    "X_v_ind = X_val.index\n",
    "X_val_s = pd.DataFrame(ss.transform(X_val))\n",
    "X_val_s.index = X_v_ind\n",
    "X_val_s.columns = X_col\n",
    "\n",
    "knn.fit(X_t_s, y_t)\n",
    "print(f\"training accuracy: {knn.score(X_t_s, y_t)}\")\n",
    "print(f\"Val accuracy: {knn.score(X_val_s, y_val)}\")\n",
    "\n",
    "y_hat = knn.predict(X_val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot_train() function just does what we did above.\n",
    "\n",
    "plot_train(X_t, y_t, X_val, y_val)\n",
    "plot_train(X_t_s, y_t, X_val_s, y_val, -2, 2, text_pos=0.1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how much that changes things.\n",
    "\n",
    "Look at points 166 and 150.  \n",
    "Look at the group 621, 143, and 191.\n",
    "\n",
    "Now let's run our classifier on scaled data and compare to unscaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          random_state=42,\n",
    "                                          test_size=0.25)\n",
    "\n",
    "# The predict_one() function prints predictions on a given point\n",
    "# (#484) for k-nn models with k ranging from 1 to 10.\n",
    "\n",
    "predict_one(X_t, X_val, y_t, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "\n",
    "X_t_s = pd.DataFrame(mm.fit_transform(X_t))\n",
    "X_t_s.index = X_t.index\n",
    "X_t_s.columns = X_t.columns\n",
    "\n",
    "X_val_s = pd.DataFrame(mm.transform(X_val))\n",
    "X_val_s.index = X_val.index\n",
    "X_val_s.columns = X_val.columns\n",
    "\n",
    "\n",
    "predict_one(X_t_s, X_val_s, y_t, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$ and the Bias-Variance Tradeoff\n",
    "> Small $k$ values leads to overfitting, but larger $k$ values tend towards underfitting\n",
    "\n",
    "![alt text](img/K-NN_Neighborhood_Size_print.png)\n",
    "> From [Machine Learning Flashcards](https://machinelearningflashcards.com/) by Chris Albon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's slowly increase k and see what happens to our accuracy scores.\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "k_scores_train = {}\n",
    "k_scores_val = {}\n",
    "\n",
    "\n",
    "for k in range(1, 20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    accuracy_score_t = []\n",
    "    accuracy_score_v = []\n",
    "    for train_ind, val_ind in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind] \n",
    "        X_v, y_v = X_train.iloc[val_ind], y_train.iloc[val_ind]\n",
    "        mm = MinMaxScaler()\n",
    "        \n",
    "        X_t_ind = X_t.index\n",
    "        X_v_ind = X_v.index\n",
    "        \n",
    "        X_t = pd.DataFrame(mm.fit_transform(X_t))\n",
    "        X_t.index = X_t_ind\n",
    "        X_v = pd.DataFrame(mm.transform(X_v))\n",
    "        X_v.index = X_v_ind\n",
    "        \n",
    "        knn.fit(X_t, y_t)\n",
    "        \n",
    "        y_pred_t = knn.predict(X_t)\n",
    "        y_pred_v = knn.predict(X_v)\n",
    "        \n",
    "        accuracy_score_t.append(accuracy_score(y_t, y_pred_t))\n",
    "        accuracy_score_v.append(accuracy_score(y_v, y_pred_v))\n",
    "        \n",
    "        \n",
    "    k_scores_train[k] = np.mean(accuracy_score_t)\n",
    "    k_scores_val[k] = np.mean(accuracy_score_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "ax.plot(list(k_scores_train.keys()), list(k_scores_train.values()),\n",
    "        color='red', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10, label='Train')\n",
    "ax.plot(list(k_scores_val.keys()), list(k_scores_val.values()),\n",
    "        color='green', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10, label='Val')\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
