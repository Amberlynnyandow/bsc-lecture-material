{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98150635",
   "metadata": {},
   "source": [
    "# Intro to Logistic Regression - our first Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abf15a",
   "metadata": {},
   "source": [
    "## Objectives \n",
    "* Understand differences between regression and classification \n",
    "* Interpret the sigmoid function \n",
    "* Use SKlearn to train and fit a logistic regression model \n",
    "* Be able to explain differences in classification metrics \n",
    "* Deal with unbalanced data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86846c8",
   "metadata": {},
   "source": [
    "![](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/machine-learning1.png?SnePeroHk5B9yZaLY7peFkULrfW8Gtaf&itok=yjEJbEKD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4bf51c",
   "metadata": {},
   "source": [
    "## What is Logistic Regression? \n",
    "\n",
    "![](https://miro.medium.com/max/400/1*zLfpo6F_Bfi6uvRL6iLX_Q.jpeg)\n",
    "It belongs to a class of predictive models called _Generalized Linear Models_. All of these models have 2 things in common: They all define significant relationships between independent/dependent variables and they indicate the strength of the relationships. \n",
    "\n",
    "Different from Linear regression -- it can predict the probabilities associated with a success or a failure. \n",
    "* Is this email likely spam? <br/>\n",
    "* What is the probability that this citizen will vote Republican? <br/>\n",
    "* Is this homeowner likely to default on their mortgage? <br/>\n",
    "* Is this person likely to buy our product? <br/>\n",
    "* Is this tumor likely to be cancerous or benign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# For demonstrative pruposes\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glass identification dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass.sort_values('al', inplace=True)\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types 1, 2, 3 are window glass\n",
    "# types 5, 6, 7 are household glass\n",
    "glass['household'] = glass.glass_type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abcb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household')\n",
    "ax.set_title('Type of Glass as a Function of Aluminum Content');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b1994",
   "metadata": {},
   "source": [
    "### If we try to fit a regressor to this data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97202c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model and store the predictions\n",
    "\n",
    "linreg = LinearRegression()\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "linreg.fit(X, y)\n",
    "glass['household_pred'] = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot that includes the regression line\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.plot(glass.al, glass.household_pred, color='red')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11a67b",
   "metadata": {},
   "source": [
    "### Interpreting Our Predictions\n",
    "If **al=3**, what class do we predict for household?\n",
    "\n",
    "If **al=1.5**, what class do we predict for household?\n",
    "\n",
    "We predict the 0 class for **lower values of al**, and the 1 class for **higher values of al**. What's our cutoff value? Around al=2, because that's where the linear regression line crosses the midpoint between predicting class 0 and class 1.\n",
    "\n",
    "Therefore, we'll say that if household_pred >= 0.5, we predict a class of 1, else we predict a class of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6103480",
   "metadata": {},
   "source": [
    "## Logistic Regression - explained \n",
    "Logistic regression can do what we just did.\n",
    "\n",
    "![](https://miro.medium.com/max/571/0*tGVPGu3aa1rhTdfl.png)\n",
    "\n",
    "The strategy now is to generalize the notion of linear regression; linear regression as we've known it will become a special case. In particular, we'll keep the idea of the regression best-fit line, but now we'll allow the model to make predictions through some (non-trivial) transformation of the linear predictor.\n",
    "\n",
    "Let's say we've constructed our best-fit line, i.e. our linear predictor, $\\hat{L} = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$.\n",
    "\n",
    "Consider the following transformation:\n",
    "$\\large\\hat{y} = \\Large\\frac{1}{1 + e^{-\\hat{L}}} \\large= \\Large\\frac{1}{1 + e^{-(\\beta_0 + ... + \\beta_nx_n)}}$. This is called the sigmoid function.\n",
    "\n",
    "We're imagining that $\\hat{L}$ can take any values between $-\\infty$ and $\\infty$.\n",
    "\n",
    "$\\large\\rightarrow$ But what values can $\\hat{y}$ take? What does this function even look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot this function here:\n",
    "\n",
    "X = np.linspace(-10, 10, 300)\n",
    "Y = 1 / (1 + np.exp(-X))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(X, Y, 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ba219",
   "metadata": {},
   "source": [
    "## More Interpretation\n",
    "This function squeezes our predictions between 0 and 1. And that's why it's so useful for binary classification problems.\n",
    "\n",
    "Suppose I'm building a model to predict whether a plant is poisonous or not, based perhaps on certain biological features of its leaves. I'll let '1' indicate a poisonous plant and '0' indicate a non-poisonous plant.\n",
    "\n",
    "Now I'm forcing my predictions to be between 0 and 1, so suppose for test plant $P$ I get some value like 0.19.\n",
    "\n",
    "I can naturally understand this as the probability that $P$ is poisonous.\n",
    "\n",
    "If I truly want a binary prediction, I can simply round my score appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2d60b",
   "metadata": {},
   "source": [
    "## Fitting Logistic Regression\n",
    "### sklearn.linear_model.LogisticRegression()\n",
    "In general, we should always scale our data when using this class. Scaling is always important for models that include regularization, and scikit-learn's LogisticRegression() objects have regularization by default.\n",
    "\n",
    "Here we've forgone the scaling since we only have a single predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the class predictions\n",
    "\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.household\n",
    "logreg.fit(X, y)\n",
    "glass['household_pred_class'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5239a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the class predictions\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(glass.al, glass.household)\n",
    "ax.plot(glass.al, glass.household_pred_class, color='red')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbce60",
   "metadata": {},
   "source": [
    "### .predict() vs. .predict_proba()\n",
    "Let's checkout some specific examples to make predictions with. We'll use both predict() and predict_proba()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e53623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine some example predictions\n",
    "\n",
    "print(logreg.predict(glass['al'][22].reshape(1, -1)))\n",
    "print(logreg.predict(glass['al'][185].reshape(1, -1)))\n",
    "print(logreg.predict(glass['al'][164].reshape(1, -1)))\n",
    "print('\\n')\n",
    "print(logreg.predict_proba(glass['al'][22].reshape(1, -1))[0])\n",
    "print(logreg.predict_proba(glass['al'][185].reshape(1, -1))[0])\n",
    "print(logreg.predict_proba(glass['al'][164].reshape(1, -1))[0])\n",
    "first_row = glass['al'][22].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee38bf",
   "metadata": {},
   "source": [
    "## Let's walk thru a more complete example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db99822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e870f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] = df['country'].replace(' ?',np.nan)\n",
    "df['workclass'] = df['workclass'].replace(' ?',np.nan)\n",
    "df['occupation'] = df['occupation'].replace(' ?',np.nan)\n",
    "\n",
    "df.dropna(how='any',inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's our dependent variable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at a countplot to visualize our new dependent variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73776f",
   "metadata": {},
   "source": [
    "**How could we adjust this countplot to visualize the relationship between sex and salary?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf1dd8",
   "metadata": {},
   "source": [
    "**Now let's dummy our objects, split our data into a features and target and aplit into training/testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c790545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['salary'], axis=1)\n",
    "y = df['salary']\n",
    "\n",
    "split_size = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=split_size,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78525968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset: {0}{1}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Test dataset: {0}{1}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ca432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance and fit the model \n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "prediction = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0805740",
   "metadata": {},
   "source": [
    "## Classification Metrics - lots of options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print('-'*40)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, prediction))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "print('-'*40)\n",
    "print('Classification Matrix:')\n",
    "print(classification_report(y_test, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305c9a6",
   "metadata": {},
   "source": [
    "## Evaluating Classification Models \n",
    "\n",
    "For classification problems, the target is a categorical variable. This means that we can simply count the number of times that our model predicts the correct category and the number of times that it predicts something else.\n",
    "\n",
    "We can visualize this by means of a **confusion matrix**, a tabular representation of Actual vs Predicted values.\n",
    "![](https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg)\n",
    "\n",
    "**The metrics for evaluating your models performance can be drawn from this matrix** \n",
    "\n",
    "* Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "* Recall = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "* Precision = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "* F-1 Score = $\\frac{2PrRc}{Pr + Rc}$ = $\\frac{2TP}{2TP + FP + FN}$ \n",
    "\n",
    "**General Lessons**: \n",
    "First, let's make some general observations about the metrics we've so far defined.\n",
    "\n",
    "**Accuracy:**\n",
    "\n",
    "   * **Pro:** Takes into account both false positives and false negatives.\n",
    "\n",
    "   * **Con:** Can be misleadingly high when there is a significant class imbalance. (A lottery-ticket predictor that always predicts a loser will be highly accurate.)\n",
    "\n",
    "**Recall:**\n",
    "\n",
    "   * **Pro:** Highly sensitive to false negatives.\n",
    "\n",
    "   * **Con:** No sensitivity to false positives.\n",
    "\n",
    "**Precision:**\n",
    "\n",
    "   * **Pro:** Highly sensitive to false positives.\n",
    "\n",
    "   * **Con:** No sensitivity to false negatives.\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160fa6c",
   "metadata": {},
   "source": [
    "## Practice: \n",
    "\n",
    "1. We are working with a dataset that is predicting disease. Consider a population with 1200 people. 5%(60 ppl) of the population has the disease. Discuss and try to answer the following questions using the formulas for precision/recall.  <br/>\n",
    "\n",
    "    a. What is the positive and negative class? <br/>\n",
    "    b. What would a recall of 80% mean? What type of error is this? <br/>\n",
    "    c. What would a precision of 80% mean? What type of error is this?\n",
    " \n",
    " \n",
    "2. We are working with an algorithm that predicts whether someone will pay back a loan or not. Out of 1000 people, 60% will pay back the loan(TP-600).<br/>\n",
    "\n",
    "    a. What is the positive class? <br/>\n",
    "    b. What would a recall of 75% mean? <br/>\n",
    "    c. What would a precision of 85% mean? <br/>\n",
    "    d. What’s more important, precision or recall? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c17f9a",
   "metadata": {},
   "source": [
    "## A couple More Metrics \n",
    "**F-1 Score:**\n",
    "\n",
    "Harmonic mean of recall and precision.\n",
    "\n",
    "**AIC (Akaike Information Criteria**) — The analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.\n",
    "\n",
    "**ROC Curve:** Receiver Operating Characteristic (ROC) summarizes the model’s performance by evaluating the trade-offs between true positive rate (sensitivity) and false positive rate (1- specificity). For plotting ROC, it is advisable to assume p > 0.5 since we are more concerned about success rate. ROC summarizes the predictive power for all possible values of p > 0.5. The area under curve (AUC), referred to as index of accuracy (A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model. Below is a sample ROC curve. The ROC of a perfect predictive model has TP equals 1 and FP equals 0. This curve will touch the top left corner of the graph.\n",
    "![](https://miro.medium.com/max/300/0*20UWoOC5Gi4SdbAw.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dcd6f8",
   "metadata": {},
   "source": [
    "### Back to our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a7fd6",
   "metadata": {},
   "source": [
    "## We have an imbalance problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefc218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train, y_train) \n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel2 = LogisticRegression(C=100, solver='liblinear')\n",
    "logmodel2.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = logmodel2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ae25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*40)\n",
    "print('Accuracy Score:')\n",
    "print(accuracy_score(y_test, prediction2))\n",
    "\n",
    "print('-'*40)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, prediction2))\n",
    "\n",
    "print('-'*40)\n",
    "print('Classification Matrix:')\n",
    "print(classification_report(y_test, prediction2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df2ff7",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "#### Just a quick intro - we'll get into this much more tomorrow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52530f16",
   "metadata": {},
   "source": [
    "#### LogisticRegression has several optional parameters that define the behavior of the model and approach: \n",
    "\n",
    "**penalty**- is a string ('l2' by default) that decides whether there is regularization and which approach to use. Other options are 'l1', 'elasticnet', and 'none'.\n",
    "\n",
    "**dual**- is a Boolean (False by default) that decides whether to use primal (when False) or dual formulation (when True).\n",
    "\n",
    "**tol**- is a floating-point number (0.0001 by default) that defines the tolerance for stopping the procedure.\n",
    "\n",
    "**C**- is a positive floating-point number (1.0 by default) that defines the relative strength of regularization. Smaller values indicate stronger regularization.\n",
    "\n",
    "**fit_intercept**- is a Boolean (True by default) that decides whether to calculate the intercept 𝑏₀ (when True) or consider it equal to zero (when False).\n",
    "\n",
    "**intercept_scaling**- is a floating-point number (1.0 by default) that defines the scaling of the intercept 𝑏₀.\n",
    "\n",
    "**class_weight**- is a dictionary, 'balanced', or None (default) that defines the weights related to each class. When None, all classes have the weight one.\n",
    "\n",
    "**random_state**- is an integer, an instance of numpy.RandomState, or None (default) that defines what pseudo-random number generator to use.\n",
    "\n",
    "**solver**- is a string ('liblinear' by default) that decides what solver to use for fitting the model. Other options are 'newton-cg', 'lbfgs', 'sag', and 'saga'.\n",
    "\n",
    "**max_iter**- is an integer (100 by default) that defines the maximum number of iterations by the solver during model fitting.\n",
    "\n",
    "**multi_class**- is a string ('ovr' by default) that decides the approach to use for handling multiple classes. Other options are 'multinomial' and 'auto'.\n",
    "\n",
    "**verbose**- is a non-negative integer (0 by default) that defines the verbosity for the 'liblinear' and 'lbfgs' solvers.\n",
    "\n",
    "**warm_start**- is a Boolean (False by default) that decides whether to reuse the previously obtained solution.\n",
    "\n",
    "**n_jobs**- is an integer or None (default) that defines the number of parallel processes to use. None usually means to use one core, while -1 means to use all available cores.\n",
    "\n",
    "**l1_ratio**- is either a floating-point number between zero and one or None (default). It defines the relative importance of the L1 part in the elastic-net regularization.\n",
    "\n",
    "#### Warning: \n",
    "**You should carefully match the solver and regularization method for several reasons:**\n",
    "\n",
    "'liblinear' solver doesn’t work without regularization. <br/>\n",
    "'newton-cg', 'sag', 'saga', and 'lbfgs' don’t support L1 regularization. <br/>\n",
    "'saga' is the only solver that supports elastic-net regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517502b",
   "metadata": {},
   "source": [
    "### Comparing Logistic Regression with Other Models\n",
    "\n",
    "Advantages of logistic regression:\n",
    "\n",
    "- Highly interpretable (if you remember how)\n",
    "- Model training and prediction are fast\n",
    "- Not many parameters to tune\n",
    "- Can perform well with a small number of observations\n",
    "- Outputs well-calibrated predicted probabilities\n",
    "\n",
    "Disadvantages of logistic regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the log-odds of the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04681e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
