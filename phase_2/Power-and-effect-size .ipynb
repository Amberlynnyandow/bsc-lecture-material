{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Power "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power is 1-β, where β is the probability of wrongly concluding there is no effect when one actually exists. This type of error is termed Type II error. Like statistical significance, statistical power depends upon effect size and sample size. If the effect size of the intervention is large, it is possible to detect such an effect in smaller sample numbers, whereas a smaller effect size would require larger sample sizes.\n",
    "\n",
    "Methods to increase the power of your study include:\n",
    "- using more potent interventions that have bigger effects \n",
    "- increasing the size of the sample/subjects(not the best solution)\n",
    "- raising the α level but only if making a Type I error is highly unlikely OR if making a type 1 error is not \"as bad\" as making a type 2 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical power of a hypothesis test is a function of:\n",
    "\n",
    "* the sample size,\n",
    "* the significance level 𝛼, and\n",
    "* the effect size or difference between the groups we are testing\n",
    "\n",
    "Typically accepted values for the power of a statistical test are greater than or equal to 0.80 or 80%. Studies with power less than 80% are said to be underpowered and require a reevaluation of experimental design or acquiring more samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect size \n",
    "\n",
    "When we design an experiment, we want to make sure to gather enough data to be able to detect differences between our groups, should the difference exist. The effect size is a measure of the difference between the two groups we're testing. \n",
    "\n",
    "Cohen's d, denoted by $d$, is a _standardized_ effect size measure equal to the magnitude of the difference in sample means divided by the pooled sample standard deviation of the two samples. \n",
    "* We use standardized effect sizes so we can remove the units of the variables in the effect size.  \n",
    "\n",
    "When testing the difference in the sample means of two samples, we use Cohen's d to measure the effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Intervention Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohen's d\n",
    "\n",
    "Cohen's d is given by: \n",
    "\n",
    "$$ \\large d = \\frac{|\\mu_2 - \\mu_1|}{s_p},  $$\n",
    "\n",
    "where $\\mu_1$ and $\\mu_2$ are the sample means for sample 1 and 2, respectively, and $s_p$ is the pooled standard deviation of the two samples. \n",
    "\n",
    "The pooled standard deviation $s_p$ of the two samples is given by: \n",
    "\n",
    "$$ \\large s_p = \\sqrt{\\frac{\\left(n_1 -1\\right)s_1^2 + \\left(n_2 -1\\right)s_2^2 }{n_1 + n_2 - 2}}, $$\n",
    "\n",
    "where $n_1$ and $n_2$ are the sample sizes for sample 1 and sample 2, respectively, and $s_1^2$ and $s_2^2$ are the sample variances for sample 1 and sample 2, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(sample1, sample2):\n",
    "    n1, n2 = len(sample1), len(sample2)\n",
    "    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)\n",
    "    pooled_var = ((n1-1)*var1 + (n2-1)*var2)/(n1+n2-2)\n",
    "    s = np.sqrt(pooled_var)\n",
    "    \n",
    "    mean1, mean2 = np.mean(sample1), np.mean(sample2)\n",
    "    \n",
    "    return np.abs(mean2-mean1)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect sizes are considered to be small, medium, or large depending on the following rule of thumb: \n",
    "\n",
    "||Cohen's d|\n",
    "|--|--|\n",
    "|small|0.2|\n",
    "|medium|0.5|\n",
    "|large|0.8|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the effect size for the following two samples, `sample1` and `sample2`, using the function for Cohen'd written above. Is this a small, medium, or large effect size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) #for reproducibility\n",
    "rv1 = stats.norm(loc=10, scale=1)\n",
    "rv2 = stats.norm(loc=12, scale=2)\n",
    "\n",
    "sample1 = rv1.rvs(25)\n",
    "sample2 = rv2.rvs(25)\n",
    "\n",
    "cohen_d(sample1, sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Large effect size. \n",
    "\n",
    "**What if the sample size was 50?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) #for reproducibility\n",
    "\n",
    "sample1 = rv1.rvs(50)\n",
    "sample2 = rv2.rvs(50)\n",
    "\n",
    "cohen_d(sample1, sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why did the effect size increase when we increased the sample size of our samples?**\n",
    "\n",
    "> The pooled standard deviation of the samples decreases with increasing sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's compute type II error rate using a simulation and compute power instead.\n",
    "\n",
    "In this example, the null hypothesis is that the two samples of scores come from the same population of scores, and the alternative hypothesis is that they do not come from the same population of scores. \n",
    "\n",
    "**When you compute the power of a statistical test, what probability are you calculating?**\n",
    "\n",
    "> You compute the probability that you'll reject the null hypothesis given it's false. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's run 1000 simulations and compute the power of our test.** \n",
    "\n",
    "Remember that $\\text{power} = 1 - \\beta$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instances of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = stats.norm(loc=5, scale=1)\n",
    "scores_population_2 = stats.norm(loc=6, scale=2)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 25 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = scores_population_1.rvs(n_sample_size), scores_population_2.rvs(n_sample_size) \n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of the number of times you reject the null hypothesis \n",
    "    if result[1] > alpha:\n",
    "        c+=1\n",
    "\n",
    "type_2_error_rate = c/n_simulations\n",
    "\n",
    "power = 1 - type_2_error_rate\n",
    "\n",
    "print(\"Power: {}\".format(power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we were limited to a sample size of 10, what would be the power of our test?** \n",
    "\n",
    "> Run code again with `sample_size = 10`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two instance of a normal continuous random variable, one with mean 5 and standard deviation 1,\n",
    "# and another with mean 6 and standard deviation 2.\n",
    "scores_population_1 = stats.norm(loc=5, scale=1)\n",
    "scores_population_2 = stats.norm(loc=6, scale=2)\n",
    "\n",
    "# Set the number of simulations to run to 1000\n",
    "n_simulations = 1000\n",
    "\n",
    "# Set the size of the samples you'll draw to 25 \n",
    "n_sample_size = 10 \n",
    "\n",
    "# You reject the null hypothesis is the p-value of the two-sided t-test is less than alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Keep count of the number of times you reject the null hypothesis \n",
    "c = 0\n",
    "\n",
    "# Run the simulations \n",
    "for i in range(n_simulations):\n",
    "    sample1, sample2 = scores_population_1.rvs(n_sample_size), scores_population_2.rvs(n_sample_size) \n",
    "    result = stats.ttest_ind(sample1, sample2)\n",
    "    \n",
    "    # Keep track of the number of times you reject the null hypothesis \n",
    "    if result[1] > alpha:\n",
    "        c+=1\n",
    "\n",
    "type_2_error_rate = c/n_simulations\n",
    "\n",
    "power = 1 - type_2_error_rate\n",
    "\n",
    "print(\"Power: {}\".format(power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As the sample size of our samples decreases, so does the power of the statistical test we perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power and effect size, sample size, and $\\alpha$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following four quantities are interrelated:\n",
    "* power\n",
    "* effect size\n",
    "* sample size\n",
    "* significance level, $\\alpha$ \n",
    "\n",
    "Given any of these three quantities, we can determine the fourth. \n",
    "\n",
    "Let's explore how power depends on effect size, sample size, and significance level $\\alpha$. \n",
    "\n",
    "The function below allows you to compute power for any `effect_size`, `sample_size` and `alpha` combination using `n_simulations` simulated tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_power(effect_size, sample_size, alpha, n_simulations=1000):\n",
    "    \n",
    "    rv1 = stats.norm(loc=0, scale=1)\n",
    "    rv2 = stats.norm(loc=effect_size, scale=1)\n",
    "    \n",
    "    # keep a count of the times you failed to reject the null hypothesis\n",
    "    c = 0\n",
    "    for i in range(n_simulations):\n",
    "        sample1, sample2 = rv1.rvs(sample_size), rv2.rvs(sample_size)\n",
    "        result = stats.ttest_ind(sample1, sample2)\n",
    "        if result[1] > alpha:\n",
    "            c+=1\n",
    "            \n",
    "    beta = c/n_simulations\n",
    "    power = 1 - beta\n",
    "    \n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens to the power of a two-sided t-test as the sample size is changed?** \n",
    "\n",
    "**Create a plot to show how power changes as sample size changes.** \n",
    "\n",
    "Use `sample_sizes = [10, 20, 50, 100]`. \n",
    "\n",
    "Assume $\\alpha=0.05$ and that you're want to measure an effect size equal to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [10, 20, 50, 100]\n",
    "powers = []\n",
    "for size in sample_sizes:\n",
    "    powers.append(get_power(0.5, size, 0.05))\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "plt.plot(sample_sizes, powers, marker='o', ls='dashed')\n",
    "plt.xlabel('sample size')\n",
    "plt.ylabel('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Power increases as sample size increases, all other things being the same. \n",
    "\n",
    "**What happens to the power of a two-sided t-test as the effect size we want to detect changes? Create a plot to show how power changes as the effect size changes.** \n",
    "\n",
    "Use `effect_sizes = [0.1, 0.2, 0.5, 0.8]`. \n",
    "\n",
    "Assume alpha=0.05 and sample_size=100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_sizes = [0.1, 0.2, 0.5, 0.8]\n",
    "powers = []\n",
    "for es in effect_sizes:\n",
    "    powers.append(get_power(es, 100, 0.05))\n",
    "\n",
    "plt.plot(effect_sizes, powers, marker='o', ls='dashed')\n",
    "plt.xlabel('effect size')\n",
    "plt.ylabel('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Power increases as the effect size increases, all other things being the same. \n",
    "\n",
    "**What happens to the power of a statistical test as $\\alpha$ changes? Create a plot to show how power changes as $\\alpha$ changes.** \n",
    "\n",
    "Use `alphas = [0.001, 0.01, 0.05, 0.1, 0.2]`. \n",
    "\n",
    "Assume sample_size=100 and that you're trying to measure an effect_size = 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "powers = []\n",
    "for alpha in alphas:\n",
    "    powers.append(get_power(0.5, 100, alpha))\n",
    "\n",
    "plt.plot(alphas, powers, marker='o', ls='dashed')\n",
    "plt.xlabel('alphas')\n",
    "plt.ylabel('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As alpha increases, power increases, all other things remaining the same.\n",
    "\n",
    "_INSIGHT:_ If you're willing to accept making more Type I error, you decrease the Type II error of the test (increase the power of the test). \n",
    "\n",
    "<details>\n",
    "    <summary>Q:What's an example of a situation where we want to prioritize Type 11 error over type 1 error?</summary> \n",
    "    - Detecting the presence of disease \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "* A Type I error is made when the null hypothesis is rejected given it's true. Type I error rate is denoted by $\\alpha$ and is equal to the significance level of our test.  \n",
    "* A Type II error is made when we fail to reject the null hypothesis given it's false. Type II error rate is denoted by $\\beta$. \n",
    "* Power is the probability of rejecting the null hypothesis given it's false. $\\text{power} = 1 - \\beta$. \n",
    "* The statistical power of a test is determined by $\\alpha$, sample size, and the effect size we're trying to measure. \n",
    "* Cohen's d is a standardized measure of the difference between two sample means. \n",
    "* We use statistical power analysis to determine the power of an already-designed experiment, or alternatively, we use statistical power analysis to determine the sample size we need to measure a given effect size in an experiment, should it exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
