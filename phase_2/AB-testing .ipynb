{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "- Explain how A/B testing relates to hypothesis testing\n",
    "- Describe the considerations in designing an A/B test\n",
    "- Explain how to conduct an A/B test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stats stuff\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to A/B Testing\n",
    "### Optimizing your website's funnel \n",
    "\n",
    "> **A/B testing** is a general methodology when you want to test a new feature and/or product (especially for online products)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is It Exactly?\n",
    "![](https://acquire.io/wp-content/uploads/2019/11/21_11_19_What-is-Conversion-Rate-in-Business_-07.png)\n",
    "\n",
    "In the online world, the number of visitors on your website equals the number of opportunities you have to expand your business by acquiring new customers and build relationships by catering to existing ones. It is your conversion funnel that decides whether your website gets good traffic and converts more visitors.\n",
    "\n",
    "Businesses want visitors to take action (also called a conversion) on their website, and the rate at which a site can drive this is called its “conversion rate.” The more optimized your funnel, the higher is the conversion rate. One of the most important ways to optimize your website’s funnel in digital marketing is A/B testing.\n",
    "\n",
    "![](https://static.wingify.com/gcp/uploads/2019/12/Why-Should-You-A-B-Test-1024x352.png?tr=w-1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling out new features and products can be exciting but also nerve-wracking! An organization hopes that new features, updates, etc. are going to be useful to their customers/users but there's the possibility they weren't as great as they hoped.\n",
    "\n",
    "A lot of times, we can make smaller, iterative improvements to a product before committing fully to making the change for all users or dedicating time committing to the new changes.\n",
    "\n",
    "What we can do is give out these changes to a relatively small group in order to gain insights on how we might want to proceed in making changes.\n",
    "\n",
    "These changes can include changes to the user-interface (UI) of a website, new recommendation models, new features, etc.\n",
    "![](images/mountains.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A/B testing is best suited for informing us what improvements we can make on something we already have. It's not really meant to tell us what _new direction_ we should go.\n",
    ">\n",
    "> An analogy: A/B testing is meant to tell us how to climb up the mountain we're already on, but not necessarily _what_ mountain we should be on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does It Relate to What We've Learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you probably realized this is related to hypothesis testing! In fact, A/B testing is essentially **hypothesis testing** but applied to a business problem!\n",
    "\n",
    "We typically will carry out _experiments_ by comparing different groups to some control group. The simplest case is comparing a group exposed to the new changes to a group where no changes were made. But you can also have _many_ different groups each with slight variations!\n",
    "\n",
    "![](https://images.ctfassets.net/zw48pl1isxmc/4QYN7VubAAgEAGs0EuWguw/165749ef2fa01c1c004b6a167fd27835/ab-testing.png)\n",
    "> Image source from [Optimizely](https://www.optimizely.com/optimization-glossary/ab-testing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have these groups, we can use an appropriate hypothesis test to determine if it is _significant_ enough to warrant making the change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/huh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is where your skills of a data scientist can shine. It's not just doing the technical stuff correctly (though it is important!), but also how you communicate your findings to stakeholders who likely have limited knowledge of statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider Whom You're Testing\n",
    "It's important to consider _whom_ you're testing! You want to ensure that the participants in your experiment will be useful when drawing conclusions as the experiment concludes.\n",
    "\n",
    "![\"Who me?\" gif happy to be chosen](images/who_me.gif)\n",
    "> _User happy you chose them for your experiment_\n",
    "\n",
    "Sometimes it's not feasible or worth the time to segment the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be Aware of Initial Launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novelty Effect\n",
    "\n",
    "> When users love the newest and latest but eventually the \"newness\" wears off\n",
    "\n",
    "![\"That Hansel's so hot right now\" gif from Zoolander](images/hansel_is_hot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Size of Experiment\n",
    "\n",
    "When talking about hypothesis tests, typically the larger the sample the better! Does the same apply for A/B testing?\n",
    "\n",
    "Well in an A/B test, there's likely more to consider beyond getting a significant result. \n",
    "\n",
    "For example, if it takes a long time to collect data from an individual then it might be a better idea to limit the number of participants in an experiment.\n",
    "\n",
    "It also might not be worth experimenting with many individuals since it might cost the business. For example, imagine if we test on a substantial number of participants but it turns out the users react _negatively_ to that change and they quit using the product completely! \n",
    "\n",
    "So we really want to ensure we determine the size of the experiment so we can still observe a significant difference without any negative effects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Need More Power! ⚡️\n",
    "\n",
    "We really need to figure out a sample size but still keep the integrity of our hypothesis test to identify a signal in our experiment.\n",
    "\n",
    "We can probably think of the opposite based on what we learned; what things change as we increase/decrease our sample size?\n",
    "\n",
    "You might've thought of confidence level (related to 𝛼) but as we saw last Thursday, the sample size can also affect 𝛽 (related to the probability of a type 2 error) and effect size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remember:**\n",
    ">\n",
    "> $\\beta$ is the probability of a type II error occurring and is generally set to $20\\%$, similar to how $\\alpha$ is the probability of a type I error occurring and generally set to $5\\%$.\n",
    ">\n",
    "> Typically, we talk about **statistical power** which is simply $1-\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a great interactive tool showing how all of these ($\\alpha$, $\\beta$, effect size, and sample size) are all related: https://rpsychologist.com/d3/nhst/\n",
    "\n",
    "![](images/power_relationship.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For A/B tests, we can specify what confidence/significance level we want ($\\alpha$), the statistical power, and what effect we want to detect to determine the sample size. Sometimes we may need to limit the number of samples because of some constraint of the test and can sacrifice one of the other values like our confidence or effect size we hope to detect.\n",
    "\n",
    "Finding one of the parameters by setting the other three is called a **power analysis**. This can sometimes be a complicated process because we have to consider what kind of statistical test we'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example A/B Test\n",
    "## The Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been hired on by a company looking to see if they can change out their UI to get more website visitors to create an account. \n",
    "\n",
    "Their innovative idea? Modify their sign-up button from pink to slightly more pink! 🤯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Our Goal?\n",
    "\n",
    "They've tasked us to figure out if it's worth them making the change. They say their developers really don't want to put in the effort unless we're confident it has an effect.\n",
    "\n",
    "***What information do we need before designing our experiment?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's What We Asked & This Is What They Said:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _How much of an increase in sign-ups is \"worth\" the change?_\n",
    "> The company says if they have an absolute increase in the conversion rate of **just $2\\%$**, it'd be worth making the change for the whole site!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _What's the time frame for this experiment?_\n",
    "> The company says we really can afford running the experiment for **a month** since they need to make a decision soon so they can focus on the next UI updates. (*I hear they're thinking of making the button a circle next!!*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _How many users visit the site per day?_\n",
    "> The company says they have about **$40,000$ unique visitors per day**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _As a baseline, about how many visitors actually end up signing-up?_\n",
    "> Turns out it was easier to look this up ourselves, and we find there's an **$8\\%$ chance** a visitor viewing the page will sign-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the Experiment \n",
    "Obviously we won't be designing the experiment before the data are collected but we should still consider what has to go into this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What would you consider before running the experiment?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Decided:\n",
    "\n",
    "- If we want to see an increase of $2\\%$ from $8\\%$ and we choose a typical power $0.8$ and a conservative $\\alpha=0.01$, we can do a _power analysis_ to find the minimum number of samples needed is about $4,700$ samples.\n",
    "- Since we're running for just a month and we have about $40,000$ visitors per day, we probably can sample a decent number of visitors without changing a lot of visitor's UI.\n",
    "- We'll have two groups; a control group that will have no change and an experiment group that will have the updated sign-up button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Time! 🧪\n",
    "Let's pretend we already collected about a month's worth of data for the control and experiment groups.\n",
    "\n",
    "We have the data aggregated in separate files for the two groups. In the file, we have a new day on each line where we recorded the number of pageviews (for the visitors assigned to a group) and the number of conversions (sign-ups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_control = pd.read_csv('data/control.csv')\n",
    "df_experiment = pd.read_csv('data/experiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Our Data\n",
    "Let's look at the data and see if we can notice anything visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax0,ax1) = plt.subplots(nrows=2, figsize=(10,6))\n",
    "\n",
    "# Views\n",
    "ax0.set_title('Views')\n",
    "sns.kdeplot(data=df_control.views, ax=ax0, label='Control')\n",
    "sns.kdeplot(data=df_experiment.views, ax=ax0, label='Experiment')\n",
    "ax0.legend()\n",
    "\n",
    "# Conversions\n",
    "ax1.set_title('Conversions')\n",
    "sns.kdeplot(data=df_control.conversions, ax=ax1, label='Control')\n",
    "sns.kdeplot(data=df_experiment.conversions, ax=ax1, label='Experiment')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Statistical Test\n",
    "Since we are looking at the **frequency of conversions from views**, we can use the $\\chi^2$ goodness-of-fit test.\n",
    "\n",
    "Chi-Square goodness of fit test is used to find out how the observed value of a given phenomena is significantly different from the expected value. \n",
    "\n",
    "So the first step is to get our data into a format of \"observed\" (experiment) vs \"expected\" (control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just sum all the days together to see an overall change\n",
    "control_views = sum(df_control.views)\n",
    "control_conv = sum(df_control.conversions)\n",
    "\n",
    "experiment_views = sum(df_experiment.views)\n",
    "experiment_conv = sum(df_experiment.conversions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = np.array([experiment_conv, experiment_views - experiment_conv])\n",
    "\n",
    "expectations = np.array([control_conv, control_views- control_conv])\n",
    "\n",
    "print('OBSERVED (expermiment):', observations)\n",
    "print('EXPECTED (control):', expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chisquare(f_obs=observations, f_exp=expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_percent = experiment_conv/experiment_views*100\n",
    "print(f'Percent Experiment Converted: {experiment_percent:.5}%')\n",
    "control_percent = control_conv/control_views*100\n",
    "print(f'Percent Control Converted: {control_percent:.5}%')\n",
    "\n",
    "print(f'Difference between experiment & control {experiment_percent-control_percent:.3}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion?\n",
    "\n",
    "We got a significant result with $99\\%$ confidence! But although we're certain the effect isn't large enough for the company to make the change (at least based on what they told us).\n",
    "\n",
    "We might break the news like this:\n",
    "\n",
    "> _We're very confident that there was an observable effect in conversions by changing the buttons color. However, the observed effect was smaller than what was stated to make the change site-wide valuable._ \n",
    ">\n",
    "> _The difference in button color was observed to increase sign-ups by an absolute amount of about $0.2\\%$. Perhaps this change can still be made valuable since we are confident that the effect was real._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Up: Fisher's Test\n",
    "\n",
    "Another option for a hypothesis test for this example is [Fisher's exact test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test). This is typically used for _small_ frequencies but has some advantages over the $\\chi^2$ test.\n",
    "\n",
    "Fisher's Test tells us that the $p$-value corresponding to our distribution is given by:\n",
    "\n",
    "$$\\Large p = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!n!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values $a$, $b$, $c$, and $d$ are given by the frequencies of a $2\\times2$ contingency table, namely:\n",
    "\n",
    "|                     |Observations, choice #1|Observations, choice #2|\n",
    "|---------------------|---------------------|---------------------|\n",
    "|Expectations, choice #1|          $a$        |          $b$        |\n",
    "|Expectations, choice #2|          $c$        |          $d$        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use our observations (experiment group) & expecations (control group) as\n",
    "# defined earlier in the lecture\n",
    "contingency_table = np.array([observations, expectations])\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SciPy's function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the slowness of the method (due to large factorials)\n",
    "result = stats.fisher_exact(contingency_table)\n",
    "x, p = result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
